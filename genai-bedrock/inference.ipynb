{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce54158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install --quiet --upgrade pip\n",
    "%pip install --quiet boto3 matplotlib aws_sdk_bedrock_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f96c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc05887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west-2\"\n",
    "PROFILE = \"default\"\n",
    "MODELS = {\n",
    "    'nova lite':    \"us.amazon.nova-lite-v1:0\",\n",
    "    'nova pro':     \"us.amazon.nova-pro-v1:0\",\n",
    "    'sonnet 3.7':   \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    'haiku 3.5':    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    'llama 70b':    \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "    'palmyra X5':   \"us.writer.palmyra-x5-v1:0\",\n",
    "    'deepseek R1':  \"us.deepseek.r1-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1867ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook will use aws services hosted in us-west-2 region\n",
      "Notebook role is arn:aws:iam::736296836507:role/Admin\n"
     ]
    }
   ],
   "source": [
    "import boto3, logging\n",
    "import time\n",
    "\n",
    "session = boto3.Session(profile_name=PROFILE, region_name=REGION)\n",
    "sts = session.client('sts')\n",
    "bedrock = session.client('bedrock-runtime')\n",
    "role = sts.get_caller_identity()['Arn']\n",
    "# Check if this is an assumed role\n",
    "if ':assumed-role/' in role:\n",
    "    parts = role.split(':')\n",
    "    account_id = parts[4]\n",
    "    role_name = parts[5].split('/')[1]\n",
    "    role = f\"arn:aws:iam::{account_id}:role/{role_name}\"\n",
    "\n",
    "print(f'The notebook will use aws services hosted in {session.region_name} region')\n",
    "print(f'Notebook role is {role}')\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self):\n",
    "        self.short = \"\"\n",
    "    def start(self, short):\n",
    "        self.short = short\n",
    "        self.start_time = time.time()\n",
    "        self.input_tokens = self.output_tokens = 0\n",
    "        self.log('starting')\n",
    "    def stop(self):\n",
    "        self.log(f\" input tokens: {self.input_tokens}\")\n",
    "        self.log(f\"output tokens: {self.output_tokens}\")\n",
    "        self.log('stopped')\n",
    "    def log(self, message):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        print(f\"{elapsed_time:5.2f}s - [{self.short}] {message}\")\n",
    "    def add_input_tokens(self, count):\n",
    "        self.input_tokens += count\n",
    "    def add_output_tokens(self, count):\n",
    "        self.output_tokens += count\n",
    "\n",
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683674b2",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Prompts & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aecc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are writing a news flash on the weather for the city asked by the user. Use the tool to obtain current\n",
    "weather information. Use narrative format, around 500 words, no bulletpoints. Only provide the output, no comments or questions.\n",
    "\n",
    "Example: \n",
    "Record heat continues to grip the metro area as temperatures soar past 95Â°F. Residents are urged to stay indoors between 11 AM and 3 PM. Local authorities have activated emergency cooling stations across the city.\n",
    "\n",
    "Meanwhile, weather radar shows an advancing storm system from the northwest. Meteorologists predict severe thunderstorms will hit the region by early evening, bringing much-needed relief but possible flooding risks.\n",
    "\n",
    "Coastal communities should prepare for rough seas tonight, with the National Weather Service warning of dangerous rip currents and waves up to 10 feet. All beach activities are suspended until further notice.\n",
    "\n",
    "Looking ahead to the weekend, we'll see a pleasant drop in temperatures. Saturday brings sunny skies perfect for outdoor activities, while Sunday may require umbrellas as scattered showers move through the area.\n",
    "\n",
    "Stay tuned for further updates as conditions develop.\n",
    "\"\"\"\n",
    "\n",
    "MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": [{\"text\": \"What's the weather like in Zurich?\"}]\n",
    "    }\n",
    "]\n",
    "PARAMETERS = {\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.5,\n",
    "    \"topP\": 0.9,\n",
    "    \"topK\": 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62de532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city):\n",
    "    return \"\"\"Heavy thunder storm expected in the afternoon in {city}. Later in the night, temperature will drop \n",
    "              drastically and thunderstorm turns into a snow blizzard. Tomorrow morning, weather improves and\n",
    "              the weather will be mostly sunny and warm.\n",
    "\n",
    "              The highest temperature will be 25 degrees and the lowest will be -5 degrees.\n",
    "              The wind will be 20 km/h and the humidity will be 80%.\n",
    "              The pressure will be 1000 hPa.\n",
    "              The visibility will be 10 m.\n",
    "              The cloudiness will be 100%.\n",
    "              The sunrise will be at 06:00 and the sunset will be at 22:00.\n",
    "              \n",
    "              Warnings: thunderstorm can disturb your pets\n",
    "    \"\"\".format(city=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5786d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Gets information about the local weather given the name of a nearby city.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city to retrieve the weather forecast for.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"city\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9956ed4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Invoke API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de96b39",
   "metadata": {},
   "source": [
    "### Nova\n",
    "\n",
    "The request schema is nearly identical between the Invoke API (streaming and non-streaming) and the Converse API. There are subtle differences related to image and video payload encoding. Because Amazon Nova Micro does not support images or videos as input, those parts of the request schema do not apply to Amazon Nova Micro. Otherwise, the request schema is the same for all Amazon Nova understanding models.\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/nova/latest/userguide/complete-request-schema.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8e35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "def nova_output(model):\n",
    "    body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"system\": [{\"text\": SYSTEM_PROMPT}],\n",
    "        \"messages\": MESSAGES,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": PARAMETERS[\"maxTokens\"],\n",
    "            \"temperature\": PARAMETERS[\"temperature\"],\n",
    "            \"topP\": PARAMETERS[\"topP\"]\n",
    "        }\n",
    "    }\n",
    "    tracker.log(f\"invoke with model {MODELS[model]}\")\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model], body=json.dumps(body)\n",
    "    )\n",
    "    response = json.loads(response.get(\"body\").read())\n",
    "    tracker.add_input_tokens(response['usage']['inputTokens'])\n",
    "    tracker.add_output_tokens(response['usage']['outputTokens'])\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade2d8",
   "metadata": {},
   "source": [
    "### Claude\n",
    "\n",
    "Anthropic trains Claude models to operate on alternating user and assistant conversational turns. When creating a new message, you specify the prior conversational turns with the messages parameter. The model then generates the next Message in the conversation.\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#model-parameters-anthropic-claude-messages-request-response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c660d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claude_output(model):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": PARAMETERS[\"maxTokens\"],\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"messages\": [{\"role\": m['role'], \"content\": m['content'][0]['text']} for m in MESSAGES],\n",
    "        \"temperature\": PARAMETERS[\"temperature\"],\n",
    "        \"top_k\": PARAMETERS[\"topK\"]\n",
    "    }\n",
    "    tracker.log(f\"invoke with model {MODELS[model]}\")\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model], body=json.dumps(body),\n",
    "    )\n",
    "    response = json.loads(response.get(\"body\").read())\n",
    "    tracker.add_input_tokens(response['usage']['input_tokens'])\n",
    "    tracker.add_output_tokens(response['usage']['output_tokens'])\n",
    "    return response['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f0b9c",
   "metadata": {},
   "source": [
    "### Llama\n",
    "There are 4 different roles that are supported by Llama text models:\n",
    "- system: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.\n",
    "- user: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n",
    "- ipython: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.\n",
    "- assistant: Represents the response generated by the AI model based on the context provided in the system, ipython and user prompts.\n",
    "\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html)\n",
    "[Meta Documentation](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81394a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_PROMPT_TEMPLATE=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system}\n",
    "\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "def llama_output(model):\n",
    "    prompt = LLAMA_PROMPT_TEMPLATE.format(system=SYSTEM_PROMPT, user=MESSAGES[0]['content'][0]['text'])\n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": PARAMETERS[\"temperature\"],\n",
    "        \"top_p\": PARAMETERS[\"topP\"],\n",
    "        \"max_gen_len\": PARAMETERS[\"maxTokens\"]\n",
    "    }\n",
    "    tracker.log(f\"invoke with model {MODELS[model]}\")\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model], body=json.dumps(body),\n",
    "    )\n",
    "    response = json.loads(response.get(\"body\").read())\n",
    "    tracker.add_input_tokens(response['prompt_token_count'])\n",
    "    tracker.add_output_tokens(response['generation_token_count'])\n",
    "    return response['generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b62aa",
   "metadata": {},
   "source": [
    "### Writer / Palmyra\n",
    "\n",
    "[Documentation of chat format](https://dev.writer.com/api-guides/chat-completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311360fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def palmyra_output(model):\n",
    "    body = {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        }] + [{\"role\": m['role'], \"content\": m['content'][0]['text']} for m in MESSAGES],\n",
    "        \"max_tokens\": PARAMETERS[\"maxTokens\"],\n",
    "        \"temperature\": PARAMETERS[\"temperature\"],\n",
    "        \"top_p\": PARAMETERS[\"topP\"]\n",
    "    }\n",
    "    tracker.log(f\"invoke with model {MODELS[model]}\")\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODELS[model], body=json.dumps(body)\n",
    "    )\n",
    "    response = json.loads(response.get(\"body\").read())\n",
    "    # print(json.dumps(response, indent=2))\n",
    "    tracker.add_input_tokens(response['usage']['prompt_tokens'])\n",
    "    tracker.add_output_tokens(response['usage']['completion_tokens'])\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22af9f",
   "metadata": {},
   "source": [
    "### Routing of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622486be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_output(model):\n",
    "    if 'nova' in model:\n",
    "        return nova_output(model)\n",
    "    elif 'llama' in model:\n",
    "        return llama_output(model)\n",
    "    elif 'sonnet' in model or 'haiku' in model:\n",
    "        return claude_output(model)\n",
    "    elif 'deepseek' in model:\n",
    "        return deepseek_output(model)\n",
    "    elif 'palmyra' in model:\n",
    "        return palmyra_output(model)\n",
    "    else:\n",
    "        return \"Unknown model\"\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b1346",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Converse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266ae61",
   "metadata": {},
   "source": [
    "When you make a Converse request with an Amazon Bedrock runtime endpoint, you can include the following fields:\n",
    "- modelId â A required parameter in the header that lets you specify the resource to use for inference.\n",
    "\n",
    "- The following fields let you customize the prompt:\n",
    "    - messages â Use to specify the content and role of the prompts.\n",
    "    - system â Use to specify system prompts, which define instructions or context for the model.\n",
    "    - inferenceConfig â Use to specify inference parameters that are common to all models. Inference parameters influence the generation of the response.\n",
    "    - additionalModelRequestFields â Use to specify inference parameters that are specific to the model that you run inference with.\n",
    "    - promptVariables â (If you use a prompt from Prompt management) Use this field to define the variables in the prompt to fill in and the values with which to fill them.\n",
    "\n",
    "- The following fields let you customize how the response is returned:\n",
    "    - guardrailConfig â Use this field to include a guardrail to apply to the entire prompt.\n",
    "    - toolConfig â Use this field to include a tool to help a model generate responses.\n",
    "    - additionalModelResponseFieldPaths â Use this field to specify fields to return as a JSON pointer object.\n",
    "    - requestMetadata â Use this field to include metadata that can be filtered on when using invocation logs.\n",
    "\n",
    "\n",
    "[Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff6317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def converse_output(model):\n",
    "    tracker.log(f'converse with model {model}')\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[model],\n",
    "        messages=MESSAGES,\n",
    "        system=[{\"text\": SYSTEM_PROMPT}],\n",
    "        inferenceConfig={\n",
    "            \"maxTokens\": PARAMETERS['maxTokens'],\n",
    "            \"temperature\": PARAMETERS['temperature'],\n",
    "            \"topP\": PARAMETERS['topP']\n",
    "        },\n",
    "        additionalModelRequestFields={\n",
    "            # \"top_k\": PARAMETERS['topK']\n",
    "        }\n",
    "    )\n",
    "    tracker.add_input_tokens(response['usage']['inputTokens'])\n",
    "    tracker.add_output_tokens(response['usage']['outputTokens'])\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d70ba4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inline agent with converse API with tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34814505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def converse_with_tool_use_output(model):\n",
    "    messages = MESSAGES.copy()\n",
    "\n",
    "    def call_tool(tool_name, tool_args):\n",
    "        tool_func = globals()[tool_name]\n",
    "        return tool_func(**tool_args)\n",
    "\n",
    "    def converse_with_llm():\n",
    "        tracker.logger('converse with tools and model {model}')\n",
    "        response = bedrock.converse(\n",
    "            modelId=MODELS[model],\n",
    "            messages=messages,\n",
    "            system=[{\"text\": SYSTEM_PROMPT}],\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": PARAMETERS['maxTokens'],\n",
    "                \"temperature\": PARAMETERS['temperature'],\n",
    "                \"topP\": PARAMETERS['topP']\n",
    "            },\n",
    "            additionalModelRequestFields={\n",
    "                # \"top_k\": PARAMETERS['topK']\n",
    "            },\n",
    "            toolConfig = TOOLS\n",
    "        )\n",
    "        tracker.add_input_tokens(response['usage']['inputTokens'])\n",
    "        tracker.add_output_tokens(response['usage']['outputTokens'])\n",
    "        return response\n",
    "    \n",
    "    def handle_tool_use(tool_use):\n",
    "        tool_name = tool_use['name']\n",
    "        tool_args = tool_use['input'] or {}\n",
    "        try:\n",
    "            tracker.log(f'calling tool {tool_name}({\", \".join([f\"{k}={v}\" for k,v in tool_args])})')\n",
    "            tool_response = call_tool(tool_name, tool_args)\n",
    "            tracker.log(f'got tool response: {tool_response}')\n",
    "            tool_result_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId': tool_use['toolUseId'],\n",
    "                            'content': [{\"text\": tool_response}]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            messages.append(tool_result_message)\n",
    "        except Exception as e:\n",
    "            tracker.log(f'error calling tool: {e}')\n",
    "            tool_result_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId': tool_use['toolUseId'],\n",
    "                            'content': [{\"text\": f'error calling tool: {e}'}],\n",
    "                            \"status\": \"error\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            messages.append(tool_result_message)\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "        # print('reason step ', i+1)\n",
    "        response = converse_with_llm()\n",
    "        output_message = response['output']['message']\n",
    "        messages.append(output_message)\n",
    "        stop_reason = response['stopReason']\n",
    "        if stop_reason == 'tool_use':\n",
    "            for c in output_message['content']:\n",
    "                if 'toolUse' in c:\n",
    "                    handle_tool_use(c['toolUse'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return messages[-1]['content'][-1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7d494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026ca640bb44b45b9817c9fbbe96625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='API:', layout=Layout(margin='10px 10px 20px 00px'), optionâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# Create output widget to display results\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_change(change):\n",
    "    model = model_dropdown.value\n",
    "    api = api_dropdown.value\n",
    "    if model and api:\n",
    "        with output_widget:\n",
    "            clear_output(wait=True)\n",
    "            tracker.start(f'{model}/{api}')\n",
    "            if api == 'invoke':\n",
    "                result = invoke_output(model)\n",
    "            elif api == 'converse':\n",
    "                result = converse_output(model)\n",
    "            elif api == 'converse-tools':\n",
    "                result = converse_with_tool_use_output(model)\n",
    "            tracker.stop()\n",
    "            display(Markdown(f\"\\n---\\n\\n{result}\"))\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=MODELS.keys(),\n",
    "    value = None,\n",
    "    description='Model:',\n",
    "    layout=widgets.Layout(margin=\"10px 10px 20px 00px\")\n",
    ")\n",
    "\n",
    "api_dropdown = widgets.Dropdown(\n",
    "    options={'Invoke API': 'invoke', 'Converse API': 'converse', 'Converse API with Tools': 'converse-tools'},\n",
    "    value = None,\n",
    "    description='API:',\n",
    "    layout=widgets.Layout(margin=\"10px 10px 20px 00px\")\n",
    ")\n",
    "\n",
    "model_dropdown.observe(on_change, names='value')\n",
    "api_dropdown.observe(on_change, names='value')\n",
    "\n",
    "# Create vertical box layout to stack widgets with increased spacing\n",
    "vbox = widgets.VBox([widgets.HBox([api_dropdown, model_dropdown]), output_widget])\n",
    "display(vbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcf5ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experimental Bedrock Client SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96bcea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_sdk_bedrock_runtime.client import BedrockRuntimeClient, ConverseInput\n",
    "from aws_sdk_bedrock_runtime.models import Message, ContentBlockText, StopReason\n",
    "from aws_sdk_bedrock_runtime.config import Config\n",
    "from smithy_aws_core.credentials_resolvers.static import StaticCredentialsResolver\n",
    "from smithy_aws_core.identity import AWSCredentialsIdentity\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed3f6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_credentials().access_key\n",
    "credentials = AWSCredentialsIdentity(\n",
    "    access_key_id=session.get_credentials().access_key,\n",
    "    secret_access_key=session.get_credentials().secret_key,\n",
    "    session_token=session.get_credentials().token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3794ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = BedrockRuntimeClient(\n",
    "        config=Config(\n",
    "            aws_credentials_identity_resolver=StaticCredentialsResolver(credentials=credentials),\n",
    "            region = \"us-east-1\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad253dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    Message(\n",
    "        role=\"user\",\n",
    "        content=[\n",
    "            ContentBlockText(\n",
    "                value=\"Create a list of 3 of the best songs from the 1980s.\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4af4b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drweb/projects/.venv/lib/python3.12/site-packages/aws_sdk_signers/signers.py:781: AWSSDKWarning: Payload signing is enabled. This may result in decreased performance for large request bodies.\n",
      "  warnings.warn(\n",
      "/var/folders/qq/r7f7lf750n57yz89ypwh97mh0000gr/T/ipykernel_34589/2249552141.py:1: RuntimeWarning: coroutine 'BedrockRuntimeClient.converse' was never awaited\n",
      "  response =  await client.converse(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "response =  await client.converse(\n",
    "    ConverseInput(\n",
    "        model_id='amazon.titan-text-express-v1', messages=messages\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7836bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere are three of the best songs from the 1980s:\\n\\n1. \"Billie Jean\" by Michael Jackson (1982)\\n2. \"Sweet Child o\\' Mine\" by Guns N\\' Roses (1988)\\n3. \"Like a Virgin\" by Madonna (1984)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output.value.content[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecab83d",
   "metadata": {},
   "source": [
    "[inline-agent with KB](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents)\n",
    "\n",
    "[agent web tools](https://github.com/build-on-aws/bedrock-agents-webscraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40a369",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68ac37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
