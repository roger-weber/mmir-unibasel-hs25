{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab60e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install --quiet --upgrade pip\n",
    "%pip install --quiet boto3 python-pptx pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5132ba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.2 Boolean Retrieval\n",
      " •  First, let's examine the case of Boolean retrieval. In this basic scenario, documents are retrieved as sets without any\n",
      "    specific order among them. In other words, the retrieval system provides a list of documents but we do not consider\n",
      "    the order in which the documents are presented when assessing the system. Later, we will explore how to expand this\n",
      "    approach and consider the order of documents in our evaluation.\n",
      " •  Precision   and recall   are the earliest and still most important measures used for the evaluation of search algorithms.\n",
      "    Precision denotes how many of the answers are relevant from a user's perspective. Recall describes the percentage of\n",
      "    retrieved and relevant answers over all relevant documents in the collection. They form the key dimensions that\n",
      "    covers the user’s interests:\n",
      "    –  Precision is the measure of having relevant documents easily accessible without the need to go through many\n",
      "       documents in the result set. In many cases, we are not interested in all relevant documents, but only a few that give\n",
      "       us the necessary information. A good search engine that offers only relevant documents is suitable for many\n",
      "       knowledge-based or fact-checking queries (e.g., students, fact-checkers).\n",
      "    –  Recall is about exploring most of the relevant documents. In such cases, we want a comprehensive overview of the\n",
      "       relevant documents and aim to minimize the number of non-relevant documents (fall-out, false positives). A good\n",
      "       search engine should retrieve most or all of the relevant documents while avoiding too many non-relevant ones in\n",
      "       the answer (e.g., patent lawyers, searches with vague criteria).\n",
      " •  Notations:\n",
      "             𝔸       Set of all documents\n",
      "             ℝ𝑄      Set of relevant documents for a query 𝑄in the collection 𝔸\n",
      "             𝔽𝑄      Set of documents retrieved by a system for query 𝑄\n",
      "\n",
      " •  Precision 𝑝, recall 𝑟, and fallout f    are then defined as (see visualization on next page):\n",
      "                               𝑝 =   𝔽𝑄  ∩ ℝ𝑄                      𝑟 =   𝔽𝑄  ∩  ℝ𝑄                      𝑓 =   𝔽𝑄  ∖ ℝ𝑄\n",
      "                                        𝔽𝑄                                  ℝ𝑄                                 𝔸 ∖  ℝ𝑄\n",
      "\n",
      "\n",
      "\n",
      "Multimedia Retrieval – 2024                                       2.2 Boolean Retrieval                                                       Page 2-4\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "\n",
    "reader = pypdf.PdfReader('02_Evaluation.pdf')\n",
    "print(reader.pages[3].extract_text(extraction_mode='layout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "941d64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RectangleObject([0, 0, 779.76, 540])\n",
      "RectangleObject([0, 0, 779.76, 540])\n",
      "RectangleObject([0, 0, 779.76, 540])\n",
      "RectangleObject([0, 0, 779.76, 540])\n",
      "RectangleObject([0, 0, 779.76, 540])\n"
     ]
    }
   ],
   "source": [
    "print(reader.pages[3].mediabox)\n",
    "print(reader.pages[3].cropbox)\n",
    "print(reader.pages[3].bleedbox)\n",
    "print(reader.pages[3].trimbox)\n",
    "print(reader.pages[3].artbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729edd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 \n",
      "12.0 Page 2\n",
      "12.0 \n",
      "12.0 -\n",
      "12.0 \n",
      "12.0 13\n",
      "12.0 \n",
      "12.0 Multimedia Retrieval \n",
      "12.0 \n",
      "12.0 –\n",
      "12.0 \n",
      "12.0  \n",
      "12.0 \n",
      "12.0 2024\n",
      "12.0 \n",
      "20.04 \n",
      "\n",
      "20.04 2.5 The Confusion Matrix\n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 2.5 The Confusion Matrix\n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 •\n",
      "12.0 \n",
      "14.04  In the introduction, we covered performance measures like mean squared error and cross\n",
      "12.0 \n",
      "14.04 -\n",
      "12.0 \n",
      "14.04 entropy loss in machine \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 learning. Now, we delve into the performance of classification methods, focusing on both binary and multi\n",
      "12.0 \n",
      "14.04 -\n",
      "12.0 \n",
      "14.04 class tasks \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 with hard assignments.\n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 •\n",
      "12.0 \n",
      "14.04  The confusion matrix is a common approach that presents correct and incorrect classifications in a tabular form, \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 enabling the extraction of various metrics to assess the performance of a method. Rows represent the predicted \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 conditions, also known as test results, while columns denote the observed actual conditions, also known as ground \n",
      "12.0 \n",
      "14.064 \n",
      "\n",
      "14.064 truth (the labels in the data sets). Let's examine the table below:\n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 –\n",
      "12.0 \n",
      "14.04  The term \"condition\" is a general description of the task's output value. For instance, it could be \"it will rain,\" \"it's a \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 dog,\" \"patient has the disease,\" \"the object belongs to the class,\" or \"the student passes the exam.\"\n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 –\n",
      "12.0 \n",
      "14.04  The \"True\" row contains all data items for which the method predicts that the item fulfills the condition. Let's \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 compare these predictions with the actual values: first, we have the \n",
      "12.0 \n",
      "14.04 True Positives (TP)\n",
      "12.0 \n",
      "14.04  where the prediction is \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 correct (matches the observation). Second, we have the \n",
      "12.0 \n",
      "14.04 False Positives (FP) \n",
      "12.0 \n",
      "14.04 where the prediction is wrong, and the \n",
      "12.0 \n",
      "14.064 \n",
      "\n",
      "14.064 method overestimates the condition.\n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 –\n",
      "12.0 \n",
      "14.04  The \"False\" row contains all data items for which the method predicts that the item does not fulfill the condition. If \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 we compare these outcomes with the actual values, we observe the \n",
      "12.0 \n",
      "14.04 True Negatives (TN)\n",
      "12.0 \n",
      "14.04  for which the prediction is \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 correct, and the \n",
      "12.0 \n",
      "14.04 False Negatives (FN)\n",
      "12.0 \n",
      "14.04  for which the prediction is wrong. In the latter case, the method is \n",
      "12.0 \n",
      "14.04 \n",
      "\n",
      "14.04 underestimating the condition.\n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 Actual Condition (as observed)\n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 Population\n",
      "12.0 \n",
      "12.0  Positive (\n",
      "12.0 \n",
      "12.0 𝑃\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0  Negative (\n",
      "12.0 \n",
      "12.0 𝑁\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 Predicted \n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 Condition \n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 (as computed)\n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 True (\n",
      "12.0 \n",
      "12.0 𝑇\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0  True Positive\n",
      "12.0 \n",
      "12.0  (\n",
      "12.0 \n",
      "12.0 𝑇𝑃\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0  False Positive\n",
      "12.0 \n",
      "12.0  (\n",
      "12.0 \n",
      "12.0 𝐹𝑃\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0 \n",
      "\n",
      "12.0 False (\n",
      "12.0 \n",
      "12.0 𝐹\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0  False Negative\n",
      "12.0 \n",
      "12.0  (\n",
      "12.0 \n",
      "12.0 𝐹𝑁\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "12.0  True Negative\n",
      "12.0 \n",
      "12.0  (\n",
      "12.0 \n",
      "12.0 𝑇𝑁\n",
      "12.0 \n",
      "12.0 )\n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 There is also “confusion” regarding the \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 placement of actual values in the \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 confusion matrix. Earlier papers display \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 the confusion matrix with actual values \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 in the columns (like here on the left side), \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 while more recent publications and \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 popular software packages use the \n",
      "12.0 \n",
      "9.984 \n",
      "\n",
      "9.984 transformed notation, placing the actual \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 values as rows. This does not change the \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 interpretation of the confusion matrix \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 but makes it difficult to read the table if \n",
      "12.0 \n",
      "9.96 \n",
      "\n",
      "9.96 it appears in the unfamiliar form.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2.5 The Confusion Matrix\n",
       "\n",
       "In the introduction, we covered performance measures like mean squared error and cross - entropy loss in machine learning. Now, we delve into the performance of classification methods, focusing on both binary and multi - class tasks with hard assignments.\n",
       "\n",
       "The confusion matrix is a common approach that presents correct and incorrect classifications in a tabular form, enabling the extraction of various metrics to assess the performance of a method. Rows represent the predicted conditions, also known as test results, while columns denote the observed actual conditions, also known as ground truth (the labels in the data sets). Let's examine the table below:\n",
       "\n",
       " - The term \"condition\" is a general description of the task's output value. For instance, it could be \"it will rain,\" \"it's a dog,\" \"patient has the disease,\" \"the object belongs to the class,\" or \"the student passes the exam.\"\n",
       "\n",
       " - The \"True\" row contains all data items for which the method predicts that the item fulfills the condition. Let's compare these predictions with the actual values: first, we have the True Positives (TP) where the prediction is correct (matches the observation). Second, we have the False Positives (FP) where the prediction is wrong, and the method overestimates the condition.\n",
       "\n",
       " - The \"False\" row contains all data items for which the method predicts that the item does not fulfill the condition. If we compare these outcomes with the actual values, we observe the True Negatives (TN) for which the prediction is correct, and the False Negatives (FN) for which the prediction is wrong. In the latter case, the method is underestimating the condition. There is also “confusion” regarding the Actual Condition (as observed) placement of actual values in the confusion matrix. Earlier papers display Positive ( 𝑃 ) Negative ( 𝑁 ) Population the confusion matrix with actual values in the columns (like here on the left side), while more recent publications and True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) popular software packages use the transformed notation, placing the actual values as rows. This does not change the Predicted False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) Condition interpretation of the confusion matrix (as computed) but makes it difficult to read the table if it appears in the unfamiliar form."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2.5 The Confusion Matrix\\n\\nIn the introduction, we covered performance measures like mean squared error and cross - entropy loss in machine learning. Now, we delve into the performance of classification methods, focusing on both binary and multi - class tasks with hard assignments.\\n\\nThe confusion matrix is a common approach that presents correct and incorrect classifications in a tabular form, enabling the extraction of various metrics to assess the performance of a method. Rows represent the predicted conditions, also known as test results, while columns denote the observed actual conditions, also known as ground truth (the labels in the data sets). Let\\'s examine the table below:\\n\\n - The term \"condition\" is a general description of the task\\'s output value. For instance, it could be \"it will rain,\" \"it\\'s a dog,\" \"patient has the disease,\" \"the object belongs to the class,\" or \"the student passes the exam.\"\\n\\n - The \"True\" row contains all data items for which the method predicts that the item fulfills the condition. Let\\'s compare these predictions with the actual values: first, we have the True Positives (TP) where the prediction is correct (matches the observation). Second, we have the False Positives (FP) where the prediction is wrong, and the method overestimates the condition.\\n\\n - The \"False\" row contains all data items for which the method predicts that the item does not fulfill the condition. If we compare these outcomes with the actual values, we observe the True Negatives (TN) for which the prediction is correct, and the False Negatives (FN) for which the prediction is wrong. In the latter case, the method is underestimating the condition. There is also “confusion” regarding the Actual Condition (as observed) placement of actual values in the confusion matrix. Earlier papers display Positive ( 𝑃 ) Negative ( 𝑁 ) Population the confusion matrix with actual values in the columns (like here on the left side), while more recent publications and True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) popular software packages use the transformed notation, placing the actual values as rows. This does not change the Predicted False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) Condition interpretation of the confusion matrix (as computed) but makes it difficult to read the table if it appears in the unfamiliar form.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output, Markdown\n",
    "import re\n",
    "\n",
    "def extract_text(page, min_length=5, footer_y=8, header_y=540, max_distance=20):\n",
    "    parts = []\n",
    "    before_y = -1\n",
    "    last_y = -1\n",
    "    last_text = \"\"\n",
    "\n",
    "    def finalize_text(texts):\n",
    "        def mark_paragraphs(text):\n",
    "            if text.startswith(\"• \"):\n",
    "                return \"\\n\\n\" + text[2:]\n",
    "            if text.startswith(\"– \"):\n",
    "                return \"\\n\\n  - \" + text[2:]\n",
    "            if text.startswith(\"---\"):\n",
    "                return \"\\n\\n\"\n",
    "            return text.strip()\n",
    "        text = \" \".join(map(mark_paragraphs, texts))\n",
    "        return re.sub(r\"  \", \" \", text.replace(\" \\n\",\"\\n\")).strip()\n",
    "\n",
    "    def cleanup(text):\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def visitor(text, cm, tm, fontDict, fontSize):\n",
    "        nonlocal last_y, last_text, before_y\n",
    "        print(f'{fontSize} {text}')\n",
    "        y = tm[5] if tm[5]>0 else before_y\n",
    "        if last_y != y or \n",
    "            if len(last_text.strip())>0:\n",
    "                parts.append((last_y, cleanup(last_text)))\n",
    "            last_y = y\n",
    "            last_text = text\n",
    "        else:\n",
    "            last_text += \" \"\n",
    "            last_text += text\n",
    "\n",
    "    def visitor_before(operator, operands, cm, tm):\n",
    "        nonlocal before_y\n",
    "        before_y = tm[5]\n",
    "\n",
    "    page.extract_text(visitor_operand_before=visitor_before, visitor_text=visitor)\n",
    "    if len(last_text.strip())>0:\n",
    "        parts.append((last_y, cleanup(last_text)))\n",
    "    sorted_parts = filter(lambda x: len(x[1])>min_length and x[0]>footer_y and x[0]<header_y, parts)\n",
    "    texts = map(lambda x: x[1], sorted(sorted_parts, key=lambda x: -x[0]))\n",
    "    return finalize_text(texts)\n",
    "\n",
    "text = extract_text(reader.pages[12])\n",
    "\n",
    "display(Markdown(text))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2f6348f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 2.4 Retrieval with Graded Relevance\n",
       "\n",
       "Until now, we have only considered binary relevance assessments for documents. However, we can also use graded relevance assessment where the grade indicates varying degrees of match between the document and the query. For example, we can introduce relevance values between 0 and 3, where 0 means \"not relevant,\" 1 means \"somewhat relevant,\" 2 means \"relevant,\" and 3 means \"highly relevant.\" The grades now influence how we assess a search method: higher grades are preferred over lower grades at the top of the rankings.\n",
       "\n",
       "The cumulative gain (CG - k) is a measure of how valuable the top - 𝑘 results are, similar to precision at 𝑘 : with 𝑟𝑒 𝑙 𝑖 denoting the graded relevance of the document at position 𝑖 . To obtain a value between 0 and 1, 𝐶 𝐺 𝑖 needs to be normalized with 𝑘 ∙ 𝑟𝑒 𝑙 m 𝑎𝑥 . In the case of binary relevance assessments, ෢ 𝐶𝐺 𝑘 is equal to precision at 𝑘 .\n",
       "\n",
       "The discounted cumulative gain (DCG - k) incorporates the ranking by penalizing relevant documents in lower ranks: The variant on the right emphasizes relevant documents more strongly than the left formula. Other variants may use different logarithm bases and slight modifications of this approach.\n",
       "\n",
       "Search results for a given query may have varying lengths, making it challenging to interpret DCG - k values and compare them across queries. To address this, we compute a normalized DCG ( nDCG - k) by first establishing an ideal ranking where documents are sorted by their graded relevance in descending order, and then calculating the DCG - k value for this ideal ranking. This yields the highest possible value, known as the ideal DCG - k (IDCG - k), which we use to normalize the DCG value of the actual result. variant: with: (over all possible rankings of documents)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'## 2.4 Retrieval with Graded Relevance\\n\\nUntil now, we have only considered binary relevance assessments for documents. However, we can also use graded relevance assessment where the grade indicates varying degrees of match between the document and the query. For example, we can introduce relevance values between 0 and 3, where 0 means \"not relevant,\" 1 means \"somewhat relevant,\" 2 means \"relevant,\" and 3 means \"highly relevant.\" The grades now influence how we assess a search method: higher grades are preferred over lower grades at the top of the rankings.\\n\\nThe cumulative gain (CG - k) is a measure of how valuable the top - 𝑘 results are, similar to precision at 𝑘 : with 𝑟𝑒 𝑙 𝑖 denoting the graded relevance of the document at position 𝑖 . To obtain a value between 0 and 1, 𝐶 𝐺 𝑖 needs to be normalized with 𝑘 ∙ 𝑟𝑒 𝑙 m 𝑎𝑥 . In the case of binary relevance assessments, \\u0de2 𝐶𝐺 𝑘 is equal to precision at 𝑘 .\\n\\nThe discounted cumulative gain (DCG - k) incorporates the ranking by penalizing relevant documents in lower ranks: The variant on the right emphasizes relevant documents more strongly than the left formula. Other variants may use different logarithm bases and slight modifications of this approach.\\n\\nSearch results for a given query may have varying lengths, making it challenging to interpret DCG - k values and compare them across queries. To address this, we compute a normalized DCG ( nDCG - k) by first establishing an ideal ranking where documents are sorted by their graded relevance in descending order, and then calculating the DCG - k value for this ideal ranking. This yields the highest possible value, known as the ideal DCG - k (IDCG - k), which we use to normalize the DCG value of the actual result. variant: with: (over all possible rankings of documents)'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output, Markdown\n",
    "import re\n",
    "\n",
    "def extract_lines(page):\n",
    "    parts = []\n",
    "    current = (-999, -999, None, \"\")\n",
    "    before_x = before_y = None\n",
    "\n",
    "    def visitor(text, cm, tm, font_dict, font_size):\n",
    "        nonlocal current, before_x, before_y\n",
    "        if len(text.strip())==0:\n",
    "            return\n",
    "        x = tm[4] if tm[4]>0 else before_x\n",
    "        y = tm[5] if tm[5]>0 else before_y\n",
    "        # print(f'{current} {(x,y,font_size)} {abs(current[1] - y) > font_size / 2 or current[1] > x} {text}')\n",
    "        if abs(current[1] - y) > font_size / 2 or current[0] > x:\n",
    "            if len(current[3].strip())>0:\n",
    "                parts.append(current)\n",
    "            current = (x, y, int(font_size), text)\n",
    "        else:\n",
    "            current = (current[0], current[1], current[2], current[3] + \" \" + text)\n",
    "\n",
    "    def visitor_before(operator, operands, cm, tm):\n",
    "        nonlocal before_x, before_y\n",
    "        before_x, before_y = tm[4], tm[5]\n",
    "\n",
    "    # get all text lines\n",
    "    page.extract_text(visitor_operand_before=visitor_before, visitor_text=visitor)\n",
    "    if len(current[3].strip())>0:\n",
    "        parts.append(current)\n",
    "    return parts\n",
    "\n",
    "def extract_parts(page):\n",
    "    def cleanup(text):\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def is_header(line):\n",
    "        return line[1] > 540\n",
    "    \n",
    "    def is_footer(line):\n",
    "        return line[1] < 8\n",
    "\n",
    "    def has_text(line):\n",
    "        return sum(1 for char in line[3] if char.lower() if 'a' <= char <= 'z') > 3\n",
    "\n",
    "    def push_current():\n",
    "        nonlocal current, parts\n",
    "        if len(current) > 0:\n",
    "            parts.append(\" \".join(current))\n",
    "        current = []\n",
    "\n",
    "    lines = extract_lines(page)\n",
    "    filtered = filter(lambda x: not(is_header(x)) and not (is_footer(x)) and has_text(x), lines)\n",
    "    current = []\n",
    "    parts = []\n",
    "    for line in filtered:\n",
    "        # print(line)\n",
    "        if line[2] == 20:\n",
    "            push_current()\n",
    "            parts.append(\"## \" + cleanup(line[3]))\n",
    "        elif line[3].startswith(\"• \"):\n",
    "            push_current()\n",
    "            current.append(cleanup(line[3][2:]))\n",
    "        elif line[3].startswith(\"– \"):\n",
    "            push_current()\n",
    "            current.append(\"  -\")\n",
    "            current.append(cleanup(line[3][2:]))\n",
    "        else:\n",
    "            current.append(cleanup(line[3]))\n",
    "    push_current()\n",
    "    return parts\n",
    "\n",
    "parts = extract_parts(reader.pages[10])\n",
    "\n",
    "display(Markdown(\"\\n\\n\".join(parts)))\n",
    "\"\\n\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9622640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Computer Science / 15731 - 01 / 2024 Multimedia Retrieval Chapter 2: Performance Evaluation Dr. Roger Weber, roger.weber@gmail.com 2.1 Introduction 2.2 Boolean Retrieval 2.3 Retrieval with Order 2.4 Retrieval with Graded Relevance 2.5 The Confusion Matrix 2.6 Optimizing Hyperparameters 2.7 Literature and Links 2.1 Introduction 2.2 Boolean Retrieval 2.3 Retrieval with Order 2.4 Retrieval with Graded Relevance 2.5 The Confusion Matrix 2.6 Optimizing Hyperparameters 2.7 Literature and Links\n",
       "\n",
       "## 2.1 Introduction\n",
       "\n",
       "So far, we have not discussed the evaluation of classification and retrieval methods. However, both use similar metrics such as precision and recall. Precision measures the percentage of relevant or correctly labeled items among those retrieved or with the same label, while recall calculates the percentage of retrieved or correctly classified items out of the set of relevant items or items with the same label.\n",
       "\n",
       "Before we define these metrics, let’s consider what we need for a retrieval benchmark. Firstly, we need a collection of documents that match the retrieval scenario. Secondly, we require multiple queries covering various aspects of the retrieval task, along with a relevance assessment of documents against these queries. Lastly, we need a performance goal that the algorithms should achieve.\n",
       "\n",
       "  - Collections and queries: examples include MS MARCO (Microsoft Machine Reading Comprehension Dataset), which contains over 500 thousand queries from Bing against millions of retrieved documents and passages. Another example is the TREC (Text REtrieval Conference) data sets, featuring 50+ queries against several thousand documents.\n",
       "\n",
       "  - Assessment: in the TREC data set, each query is assessed against the collection of retrieved documents (only the ones returned by competing systems). Even though we do not assess all documents for each query, we obtain a relatively dense assessment. On the other hand, assessments for MS MARCO are sparse, with only a few documents assessed against the 500 thousand queries to keep the assessment efforts low. The impact of these different assessment approaches will be visualized on the next page.\n",
       "\n",
       "  - Performance goal: in web retrieval, users typically focus on the top result or the first 10 documents. The performance goal is to have a relevant document at the top of the ranking. On the other hand, a patent lawyer or a researcher aims to retrieve as many relevant documents as possible with only a few non - relevant items. They want more documents and at the same time reduce the fall - out, i.e., returned documents that turn out to be not relevant and thus incur overhead going through the results.\n",
       "\n",
       "In a typical text retrieval competition, each contestant evaluates all queries against the collection. The competition assess the combined set of documents retrieved by all contestants (often with the help of the contestants), leading to a dense assessment, as shown on the right side. Even though not all documents are assessed, this approach maintains the relative order of competing algorithms. To illustrate, if we have a relevant document that was not assessed (see arrow), it may lead to a slight overestimation of the algorithms’ ability to retrieve relevant documents (recall). However, including assessments for these documents would not alter the relative ranking of the methods.\n",
       "\n",
       "Competitions like MS MARCO have a large number of queries, making dense assessments impractical. Instead, they only assess a few documents per query (sometimes just 2 - 5 documents), leading to a sparse assessment, as shown on the right side. This significantly differs from the dense assessment above. For instance, there could be assessed and relevant documents that none of the contestants found. However, the challenge arises from missing assessment for retrieved documents, which can negatively impact the performance evaluation. For example, consider the area highlighted by the arrow: it contains relevant documents for which assessments are missing. Consequently, even if a competing algorithm provides a good answer, due to missing relevancy assessment, it may not receive credit for it. all documents true relevant assessed & relevant retrieved by any contestant all documents true relevant retrieved by any contestant assessed & relevant missing assessments impact relative ranking missing assessments but no impact on ranking\n",
       "\n",
       "## 2.2 Boolean Retrieval\n",
       "\n",
       "First, let's examine the case of Boolean retrieval. In this basic scenario, documents are retrieved as sets without any specific order among them. In other words, the retrieval system provides a list of documents but we do not consider the order in which the documents are presented when assessing the system. Later, we will explore how to expand this approach and consider the order of documents in our evaluation.\n",
       "\n",
       "Precision and recall are the earliest and still most important measures used for the evaluation of search algorithms. Precision denotes how many of the answers are relevant from a user's perspective. Recall describes the percentage of retrieved and relevant answers over all relevant documents in the collection. They form the key dimensions that covers the user’s interests:\n",
       "\n",
       "  - Precision is the measure of having relevant documents easily accessible without the need to go through many documents in the result set. In many cases, we are not interested in all relevant documents, but only a few that give us the necessary information. A good search engine that offers only relevant documents is suitable for many knowledge - based or fact - checking queries (e.g., students, fact - checkers).\n",
       "\n",
       "  - Recall is about exploring most of the relevant documents. In such cases, we want a comprehensive overview of the relevant documents and aim to minimize the number of non - relevant documents (fall - out, false positives). A good search engine should retrieve most or all of the relevant documents while avoiding too many non - relevant ones in the answer (e.g., patent lawyers, searches with vague criteria).\n",
       "\n",
       "Notations:\n",
       "\n",
       "Precision 𝑝 , recall 𝑟 , and fallout f are then defined as (see visualization on next page): 𝔸 Set of all documents ℝ 𝑄 Set of relevant documents for a query 𝑄 in the collection 𝔸 𝔽 𝑄 Set of documents retrieved by a system for query 𝑄\n",
       "\n",
       "Visualization or precision and recall Collection of Documents Relevant Documents Retrieved Documents Relevant, Retrieved Precision: 𝑝 = Recall: 𝑟 = Fallout: 𝑓 =\n",
       "\n",
       "  - The F - Measure combines precision and recall into a single value, simplifying the comparison of different retrieval methods. The parameter 𝛽 determines the importance of recall over precision. When 𝛽 =0 , only precision is considered; when 𝛽 =∞ , only recall is considered. The 𝑭 𝟏 - score is a common choice with 𝛽 =1 and is also frequently used in machine learning tasks to optimize hyperparameters (see later in this chapter). Generally, 𝛽 should be selected thoughtfully depending on the retrieval task's performance goal. For example, for fact - checking tasks, precision is prioritized over recall, making a smaller 𝛽 =0.5 suitable. On the other hand, a patent lawyer may choose 𝛽 =2 to emphasize the importance of retrieving many relevant documents while maintaining reasonable precision.\n",
       "\n",
       "Example: Comparing two methods (query has a total of 20 relevant documents) Search Method A Search Method B non - relevant document relevant document = F - score\n",
       "\n",
       "Typically, we have multiple queries, and for each query, we calculate a precision - recall pair. To evaluate the overall retrieval performance, we need to compute average precision and recall. Let 𝑁 represent the number of queries. For each query 𝑄 𝑖 , we have a set 𝔽 𝑖 retrieved by the search method and a set ℝ 𝑖 of relevant documents for that query. We use 𝑝 𝑖 and 𝑟 𝑖 to denote the precision and recall values, respectively, as explained earlier.\n",
       "\n",
       "With macro evaluation , we simply compute the average over all precision and recall values as follows: While the macro evaluation method is generally effective, it can have limitations when dealing with varying sizes of relevant documents for queries. For example, consider a query 𝑄 𝑖 that has only one relevant document in the entire collection while the other queries have hundreds of relevant documents. Not finding that relevant document for 𝑄 𝑖 would result in a precision value 𝑝 𝑖 = 0 . This can significantly lower the average precision, even if the method performs well and produces high precision values for the other queries.\n",
       "\n",
       "Micro evaluation overcomes this issue by summing the true positives and the retrieved/relevant documents before calculating the average precision and recall. This ensures a fair evaluation regardless of the set sizes of queries: With the example from above, the impact on missing out on the relevant document 𝑄 𝑖 is now much smaller and may better suit the retrieval benchmark’s design\n",
       "\n",
       "The choice between micro and macro evaluation depends on the nature of the retrieval task and the importance given to different queries. Micro evaluation tends to emphasize the performance on queries with more relevant documents since they contribute more to the overall counts. Macro evaluation, on the other hand, gives equal weight to each query, regardless of its size or importance, providing a more balanced view of the overall performance across all queries.\n",
       "\n",
       "## 2.3 Retrieval with Order\n",
       "\n",
       "In many retrieval scenarios, the order of results is crucial. For example, for web search engines or fact - checking tasks, users expect the answer to appear among the top results. In such cases, the Mean Reciprocal Rank (MRR) is the preferred metric. MRR is especially useful when dealing with benchmarks that have sparse assessments as it prevents meaningful calculations of precision and recall values. The definition for queries 𝑄 𝑖 ∈ ℚ is as follows: with 𝑟𝑎𝑛 𝑘 𝑖 being the rank of the first relevant document for query 𝑄 𝑖 . Unlike precision and recall, MRR only focuses on the first relevant document and ignores the rest. It places a high importance on the top position in the ranking and assigns lower importance to later positions, converging quickly to 0 as the rank increases.\n",
       "\n",
       "Example: consider the search method A and its results for queries 𝑄 1 to 𝑄 4 , as shown in the visualization below. The Mean Reciprocal Rank (MRR) is calculated as the average of the reciprocal ranks 𝑅 𝑅 𝑖 for all the queries 𝑄 𝑖 . Search Method A\n",
       "\n",
       "We can extend the definition of precision and recall to include the ranking of retrieved documents. The Precision - Recall Curve only considers the top - 𝑘 results in the ranking for 𝑘 = 1 . . 𝑛 (as shown in the picture below on the left side). For each top - 𝑘 result, it calculates the precision and recall values based on their relevance assessment. In the example below with k = 3 , the precision is 𝑝 3 = 2 / 3 , as 2 out of 3 documents in the top - 3 are relevant. If we have 5 relevant documents overall in the collection, then the recall is 𝑟 3 = 2 / 5 , as the top - 3 contains 2 out of 5 relevant documents. We can calculate all the other precision 𝑝 𝑖 and recall 𝑟 𝑖 values, forming the precision - recall curve as depicted on the right side below.\n",
       "\n",
       "  - As we increase k, the precision increases when the next document is relevant and decreases if it is not relevant. On the other hand, recall values only increase whenever we find a new relevant document. This results in a characteristic \"sawtooth\" plot, which is often interpolated to simplify subsequent calculations such as the area under the precision - recall curve (blue area in the picture on the right side) ranking Precision Recall Original Interpoliert 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Precision Recall Original Interpoliert I nterpolated Precision - Recall - Curve\n",
       "\n",
       "Note that we can always achieve a recall value of 1 if we keep enumerating documents in the list until all relevant ones have been returned. Once we have all the relevant documents, the PR - curve becomes complete, and we can use it directly for visual comparisons between two methods or calculate simpler metrics for the comparison.\n",
       "\n",
       "  - Near the point where recall is 0 and precision is 1, the PR - curve shows how well a method can answer fact - checker type queries where high precision for the top - 10 documents is expected but recall does not really matter.\n",
       "\n",
       "  - Near recall values of 1, the PR - curve identifies methods that can find most of the relevant documents in the collection. High precision values are preferred as they indicate low overhead when going through the result list.\n",
       "\n",
       "  - The ideal system would achieve a precision and recall value of 1. The system efficiency is measured by calculating the distance 𝑑 of the PR - curve to this ideal point. The system efficiency 𝐸 is then given by 𝐸 = 1 − 𝑑 / 2 .\n",
       "\n",
       "  - The precision at k ( P@k ) is a commonly used measure, calculated as the precision 𝑝 𝑘 of the top - 𝑘 results. It is often used when we are not interested in all relevant documents and, thus, do not consider recall values.\n",
       "\n",
       "  - Similarly, the R - precision measures the precision once a threshold recall value 𝑟 𝑡 is reached. For example, with 𝑟 𝑡 = 20% , the metric evaluates the precision once 1/5th of the relevant documents were found. This method requires knowing the total number of relevant documents in the collection.\n",
       "\n",
       "  - Finally, the average precision (AP) measures the area under the PR - curve (blue area, formula on the right side). High AP values indicate that a method maintains high precision as more and more relevant documents are found.\n",
       "\n",
       "  - By iterating over a set of queries ℚ , we can easily calculate the mean values for all the measures introduced above. The formula on the lower right side shows an example for the mean average precision (MAP) . Precision Recall System Efficiency fact - checker patent lawyer R - precision\n",
       "\n",
       "## 2.4 Retrieval with Graded Relevance\n",
       "\n",
       "Until now, we have only considered binary relevance assessments for documents. However, we can also use graded relevance assessment where the grade indicates varying degrees of match between the document and the query. For example, we can introduce relevance values between 0 and 3, where 0 means \"not relevant,\" 1 means \"somewhat relevant,\" 2 means \"relevant,\" and 3 means \"highly relevant.\" The grades now influence how we assess a search method: higher grades are preferred over lower grades at the top of the rankings.\n",
       "\n",
       "The cumulative gain (CG - k) is a measure of how valuable the top - 𝑘 results are, similar to precision at 𝑘 : with 𝑟𝑒 𝑙 𝑖 denoting the graded relevance of the document at position 𝑖 . To obtain a value between 0 and 1, 𝐶 𝐺 𝑖 needs to be normalized with 𝑘 ∙ 𝑟𝑒 𝑙 m 𝑎𝑥 . In the case of binary relevance assessments, ෢ 𝐶𝐺 𝑘 is equal to precision at 𝑘 .\n",
       "\n",
       "The discounted cumulative gain (DCG - k) incorporates the ranking by penalizing relevant documents in lower ranks: The variant on the right emphasizes relevant documents more strongly than the left formula. Other variants may use different logarithm bases and slight modifications of this approach.\n",
       "\n",
       "Search results for a given query may have varying lengths, making it challenging to interpret DCG - k values and compare them across queries. To address this, we compute a normalized DCG ( nDCG - k) by first establishing an ideal ranking where documents are sorted by their graded relevance in descending order, and then calculating the DCG - k value for this ideal ranking. This yields the highest possible value, known as the ideal DCG - k (IDCG - k), which we use to normalize the DCG value of the actual result. variant: with: (over all possible rankings of documents)\n",
       "\n",
       "Example: let’s consider the first 10 documents of a search result with graded relevance values between 0 and 3\n",
       "\n",
       "  - The table above displays 10 results, with the 2 nd column indicating the graded relevance 𝑟𝑒 𝑙 𝑘 for each document at position 𝑘 . The cumulative gain 𝐶 𝐺 𝑘 is the sum of these values up to position 𝑘 . To obtain the normalized version, we divide 𝐶 𝐺 𝑘 by 3 ∙ 𝑘 . Consequently, we obtain ෢ 𝐶𝐺 10 = 0 . 5 . For comparison, if we consider any 𝑟𝑒 𝑙 𝑘 > 0 as relevant, then the “precision at 10” is 0.70 overrating the documents with low relevance grades.\n",
       "\n",
       "  - The discounted cumulative gain 𝐷𝐶 𝐺 𝑘 is shown in the next columns: first, we have the discount factors for each ranking position. By multiplying them with the graded relevance 𝑟𝑒 𝑙 𝑘 and summing them up to position 𝑘 , we obtain the 𝐷𝐶 𝐺 𝑘 values. Since we have not normalized the relevance values, they are difficult to interpret.\n",
       "\n",
       "  - For the normalized DCG, we assume there are a total of 5 documents with a graded relevance of 3 and 10 documents with a graded relevance of 2 in the collection. The \"ideal 𝑟𝑒 𝑙 𝑘 \" column shows an ideal ranking for this scenario, allowing us to compute the ideal DCG ( 𝐼𝐷𝐶 𝐺 𝑘 ) values and use them to obtain the normalized DCG ( 𝑛𝐷𝐶 𝐺 𝑘 ) values in the rightmost column. An 𝑛𝐷𝐶 𝐺 10 value of 0.49 illustrates solid performance in this example. ideal\n",
       "\n",
       "## 2.5 The Confusion Matrix\n",
       "\n",
       "In the introduction, we covered performance measures like mean squared error and cross - entropy loss in machine learning. Now, we delve into the performance of classification methods, focusing on both binary and multi - class tasks with hard assignments.\n",
       "\n",
       "The confusion matrix is a common approach that presents correct and incorrect classifications in a tabular form, enabling the extraction of various metrics to assess the performance of a method. Rows represent the predicted conditions, also known as test results, while columns denote the observed actual conditions, also known as ground truth (the labels in the data sets). Let's examine the table below:\n",
       "\n",
       "  - The term \"condition\" is a general description of the task's output value. For instance, it could be \"it will rain,\" \"it's a dog,\" \"patient has the disease,\" \"the object belongs to the class,\" or \"the student passes the exam.\"\n",
       "\n",
       "  - The \"True\" row contains all data items for which the method predicts that the item fulfills the condition. Let's compare these predictions with the actual values: first, we have the True Positives (TP) where the prediction is correct (matches the observation). Second, we have the False Positives (FP) where the prediction is wrong, and the method overestimates the condition.\n",
       "\n",
       "  - The \"False\" row contains all data items for which the method predicts that the item does not fulfill the condition. If we compare these outcomes with the actual values, we observe the True Negatives (TN) for which the prediction is correct, and the False Negatives (FN) for which the prediction is wrong. In the latter case, the method is underestimating the condition. Actual Condition (as observed) Population Positive ( 𝑃 ) Negative ( 𝑁 ) Predicted Condition (as computed) True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) There is also “confusion” regarding the placement of actual values in the confusion matrix. Earlier papers display the confusion matrix with actual values in the columns (like here on the left side), while more recent publications and popular software packages use the transformed notation, placing the actual values as rows. This does not change the interpretation of the confusion matrix but makes it difficult to read the table if it appears in the unfamiliar form.\n",
       "\n",
       "Based on this basic table, we can derive further metrics. To this end, let us introduce the following notation:\n",
       "\n",
       "  - Based on the ground truth, 𝑃 and 𝑁 represent the number of positive and negative items, respectively. 𝑃 + 𝑁 is the total size of the dataset. In the 2x2 confusion matrix, the values in the columns sum up to 𝑃 and 𝑁 .\n",
       "\n",
       "  - Based on the predicted values, 𝑇 and 𝐹 represent the number of “true” and “false” outputs, respectively. 𝑇 + 𝐹 is the total size of the dataset. In the 2x2 confusion matrix, the values in the rows sum up to 𝑇 and 𝐹 .\n",
       "\n",
       "  - The values in the cells correspond to the notion introduced on the previous page: True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN).\n",
       "\n",
       "Let’s first consider the rows in more detail:\n",
       "\n",
       "  - The Positive Predictive Value (PPV) , or Precision , is calculated as the ratio 𝑇𝑃 / 𝑇 . It represents the proportion of correctly predicted positive items. In the context of a disease test, it indicates the percentage of people with positive (‘true’) test results who are actually sick. A low PPV value means that the method would wrongly diagnose a disease from which the patient does not actually suffer. The False Discovery Rate (FDR) is the complement of PPV and is computed as FP / 𝑇 = 1 − 𝑃𝑃𝑉 .\n",
       "\n",
       "  - The Negative Predicative Value (NPV) is calculated as the ratio 𝑇𝑁 / 𝐹 . It represents the proportion of correctly predicted negative items. In the context of a disease test, it indicates how well a method can exclude a disease during diagnostics of symptoms. A low NPV value means that the method misses many sick people. The False Omission Rate (FOR) is the complement of NPV and is computed as 𝐹𝑁 / 𝐹 = 1 − 𝑁𝑃𝑉 . Actual Condition (as observed) Population Positive ( 𝑃 ) Negative ( 𝑁 ) Predicted Condition (as computed) True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) Positive Predictive Value ( 𝑃𝑃𝑉 ), Precision False Discovery False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) False Omission Negative Predictive Value ( 𝑁𝑃𝑉 )\n",
       "\n",
       "Next, we look at the columns for further insights:\n",
       "\n",
       "  - The True Positive Rate (TPR) , also known as Recall , is calculated as 𝑇𝑃 / 𝑃 . It represents the proportion of correctly predicted items among all positive cases. This measure is often referred to as Sensitivity , for example in the context of a disease test, as it indicates the percentage of actual sick people the test can detect. A high sensitivity in a disease test effectively “rules out” the disease in negative predictions, as it rarely misdiagnoses those who have the disease. However, a high sensitivity does not necessarily indicate the ability to \"rule in\" the disease. For example, a fake test that always returns positive results will have a sensitivity of 100%, but it is not effective. The False Negative Rate (FNR) is the complement of TPR and is computed as 𝐹𝑁 / 𝑃 = 1 − 𝑇𝑃𝑅 .\n",
       "\n",
       "  - The True Negative Rate (TNR) , also known as Specificity , is calculated as 𝑇𝑁 / 𝑁 . It represents the proportion of correctly predicted items among all negative cases. This measure, for example in the context of a disease test, indicates the percentage of healthy people the test correctly classifies as \"not sick\" (false). A high specificity in a disease test effectively “rules in” the disease in positive predictions, as it rarely diagnoses the disease for healthy people. However, a high specificity does not necessarily indicate the ability to “rule out” the disease. For example, a fake test that always returns negative results will have a specificity of 100% but it is not effective. The False Positive Rate (FPR) , or Fall - Out , is the complement of TNR and is computed as 𝐹𝑃 / 𝑁 = 1 − 𝑇𝑁𝑅 . Actual Condition (as observed) Population Positive ( 𝑃 ) Negative ( 𝑁 ) Predicted Condition (as computed) True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) True Positive Rate ( 𝑇𝑃𝑅 ), Sensitivity, Recall, Hit Rate False Positive Rate ( 𝐹𝑃𝑅 ), Fall - Out False Negative Rate ( 𝐹𝑁𝑅 ), Miss Rate True Negative Rate ( 𝑇𝑁𝑅 ), Specificity\n",
       "\n",
       "Finally, we consider the diagonals of the confusion matrix:\n",
       "\n",
       "  - Accuracy (ACC) is calculated as ( 𝑇𝑃 + 𝑇𝑁 ) / ( 𝑃 + 𝑁 ) . It represents the percentage of correctly predicted items and has become a standard measure for many classification tasks in machine learning. However, high accuracy alone is not always a good indicator, as we will discuss later with an example.\n",
       "\n",
       "  - The Error Rate (ERR) , or Misclassification Rate , is the complement of the accuracy and measures the percentage of wrongly predicted items. It is calculated as ( 𝐹𝑃 + 𝐹𝑁 ) / ( 𝑃 + 𝑁 ) = 1 − 𝐴𝐶𝐶 .\n",
       "\n",
       "The literature has produced many more measures around the confusion matrix. A few examples include:\n",
       "\n",
       "  - The Prevalence is the ratio of 𝑃 over the total population. In a balanced scenario where positive and negative cases are about equally frequent, 𝑃 is close to 0.5. An extreme value for 𝑃 (<0.1, >0.9) often suggests revisiting the applicability of some of the metrics, as we will see in examples later on.\n",
       "\n",
       "  - The 𝐹 1 - score, as we introduced earlier in this chapter, is a harmonic mean between precision and recall. It is computed as 𝐹 1 = 2 ∙ 𝑃 ∙ 𝑅 / ( 𝑃 + 𝑅 ) = 2𝑇𝑃 / ( 2𝑇𝑃 + 𝐹𝑃 + 𝐹𝑁 ) . Note how the 𝐹 1 - score does not take the true negative values 𝑇𝑁 into account. The 𝐹 1 - score is widely used in the natural language processing literature for tasks such as word segmentation or entity recognition. Actual Condition (as observed) Population Positive ( 𝑃 ) Negative ( 𝑁 ) Predicted Condition (as computed) True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) Accuracy ( 𝐴𝐶𝐶 ) Error Rate ( 𝐸𝑅𝑅 ), Misclassification Rate\n",
       "\n",
       "Overview of popular metrics based on the confusion matrix for binary classifications:\n",
       "\n",
       "  - To help remember the formulae below better, the core 2x2 matrix of the confusion matrix results from counting the predictions and comparing them with the actual values. 𝑃 , 𝑁 , 𝑇 , and 𝐹 are the sums of their respective column and row. The cells in the 2x2 matrix to the right are calculated by taking the ratio of the value of the core cell in the same position and the row's total 𝑇 or 𝐹 . The rows of this matrix sum up to 1. Similarly, the cells in the 2x2 matrix below are calculated by taking the ratio of the value of the core cell in the same position and the column's total 𝑃 or 𝑁 . The columns of this matrix sum up to 1. Accuracy and Error Rate are the ratios of the sum of the diagonals and the total number of cases ( 𝑃 + 𝑁 = 𝑇 + 𝐹 ). Actual Condition (as observed) Population Positive ( 𝑃 ) Negative ( 𝑁 ) Predicted Condition (as computed) True ( 𝑇 ) True Positive ( 𝑇𝑃 ) False Positive ( 𝐹𝑃 ) Positive Predictive Value ( 𝑃𝑃𝑉 ), Precision False Discovery False ( 𝐹 ) False Negative ( 𝐹𝑁 ) True Negative ( 𝑇𝑁 ) False Omission Negative Predictive Value ( 𝑁𝑃𝑉 ) True Positive Rate ( 𝑇𝑃𝑅 ), Sensitivity, Recall, Hit Rate False Positive Rate ( 𝐹𝑃𝑅 ), Fall - Out Accuracy ( 𝐴𝐶𝐶 ) False Negative Rate ( 𝐹𝑁𝑅 ), Miss Rate True Negative Rate ( 𝑇𝑁𝑅 ), Specificity Error Rate ( 𝐸𝑅𝑅 ), Misclassification Rate\n",
       "\n",
       "Example 1: Is this a good cancer test?\n",
       "\n",
       "The prevalence value of P=30/2030=1.4% for this data set indicates a highly unbalanced distribution of positive and negative cases. This strongly suggests that we examine different values before drawing conclusions.\n",
       "\n",
       "  - We observe a high accuracy value of 91%, which seems good at first. However, on closer examination, we find that the precision (PPV) is only 10%. Out of 200 positive test results, only 20 people actually had cancer. This means that 180 people would be wrongly classified as having cancer if we follow the test results. The high accuracy is primarily due to the large number of true negatives. Interestingly, we could achieve an even higher accuracy (99%) by using a test method that always returns false, as this leads to 2000 true negatives and 0 true positives in this case.\n",
       "\n",
       "  - The low precision already indicates that we should not rely on a positive test outcome. However, when the test is negative, it is correct in 99% of the cases (NPV). Furthermore, we observe a specificity of 91% (TNR). In other words, during the diagnostic process, we can successfully rule out cancer for 91% of the patients. If this is an affordable test, it can be used as an initial step to eliminate the possibility of this cancer type.\n",
       "\n",
       "  - We might be concerned about the low sensitivity value of 67% (TPR, recall). Using only this test would miss one - third of the positive cases. Therefore, a doctor should consider other factors like symptoms or additional test results before reaching a conclusion. However, we would not recommend this test for a widely applied preventive campaign, as it could result in many false positives and unnecessary alerts, while still missing many positive cases. Actual Condition (as observed) Population ( 2030 ) Positive ( 𝑃 = 30 ) Negative ( 𝑁 = 2000 ) Predicted Condition (as computed) True ( 200 ) True Positive ( 𝑇𝑃 = 20 ) False Positive ( 𝐹𝑃 = 180 ) 𝑃𝑃𝑉 = False ( 1830 ) False Negative ( 𝐹𝑁 = 10 ) True Negative ( 𝑇𝑁 = 1820 ) 𝑁𝑃𝑉 =\n",
       "\n",
       "Example 2: Can this test prevent further spreading of a contagious virus in its early stages?\n",
       "\n",
       "The prevalence value of P=700/99,300=0.7% for this dataset shows a highly unbalanced distribution of positive and negative cases. As in the previous example, let's examine the details. Additionally, let's assume that contagious individuals need to isolate themselves, and only those with severe symptoms require further medical attention.\n",
       "\n",
       "  - Once more, we see a high accuracy of 95% but a low precision of 11%. The test incorrectly indicates a positive result for many people who do not have the virus. Unlike the first example, a false positive in this case is not as impactful for the individual (unnecessary isolation compared to unnecessary cancer treatment).\n",
       "\n",
       "  - With a high specificity of 95% (TNR), the test is effective in correctly identifying a large portion of people who do not carry the virus. However, as discussed earlier, about 5% of individuals may still be unnecessarily required to enter isolation.\n",
       "\n",
       "  - The sensitivity value is crucial in this case. We observe a fairly good value of 85% (TPR), indicating that most people carrying the virus are correctly identified. However, whether the test is acceptable depends on other factors. If the virus is highly contagious, an 85% sensitivity may not be sufficient. As discussed later in this chapter, we may need to adjust certain parameters of the test to increase sensitivity at the cost of lower specificity. This means accepting more false positives in return for reducing false negatives.\n",
       "\n",
       "  - To enhance the test's sensitivity, we can set a minimum threshold for specificity (e.g., it must be >90%), or we can create a weighted sum of specificity and sensitivity to optimize the test as we make adjustments. Actual Condition (as observed) Pop. ( 100 , 000 ) Positive ( 𝑃 = 700 ) Negative ( 𝑁 = 99 , 300 ) Predicted Condition (as computed) True ( 5 , 560 ) True Positive ( 𝑇𝑃 = 595 ) False Positive ( 𝐹𝑃 = 4 , 965 ) 𝑃𝑃𝑉 = False ( 94 , 440 ) False Negative ( 𝐹𝑁 = 105 ) True Negative ( 𝑇𝑁 = 94 , 335 ) 𝑁𝑃𝑉 =\n",
       "\n",
       "In many classification tasks, there are multiple classes, such as labeling images with recognized animals or objects. The generalized confusion matrix compares each pair of the actual class and recognized class, forming an 𝐾 × 𝐾 table where 𝐾 is the number of classes. Each cell represents the number of items with the actual class in the column and the recognized (or predicted) class in the rows. Some newer literature and software packages may transpose the table, but it does not affect the interpretation of the result. Let's consider the example below with 3 classes: \"Woman\", \"Man\", \"Child\":\n",
       "\n",
       "  - The table's diagonal represents the correctly recognized classes, while all other cells indicate the prediction errors. For instance, out of the 20 women, 13 were correctly recognized. On the other hand, 2 women were wrongly recognized as men, and 5 women were wrongly recognized as children. To visualize correct and wrong classifications, we use different colors, which make it easier to identify areas of \"confusion,\" especially with a large number of classes. Rearranging columns and rows to create \"clusters of confusion\" further helps pinpoint issues in the applied method\n",
       "\n",
       "  - Accuracy is calculated by taking the sum of the cells on the diagonal and dividing it by the total population. In this example, 𝐴𝐶𝐶 = ( 13 + 15 + 57 ) / 100 = 85% . On the other hand, the error rate is the complement of 𝐴𝐶𝐶 , giving us 𝐸𝑅𝑅 = 1 − 𝐴𝐶𝐶 = 15% for the example shown\n",
       "\n",
       "  - But how do we calculate sensitivity, specificity, precision, and other metrics from the binary confusion matrix when dealing with multiple classes? We can use a simple trick: if we want to focus on a particular class 𝐶 𝑘 , we can collapse the multi - class view into a binary view with new conditions “ ∈ 𝐶 𝑘 ” and “ ∉ 𝐶 𝑘 ” and then compute the measures as introduced before. Let’s consider an example on the next page Actual Class Population ( 100 ) Woman ( 20 ) Man ( 20 ) Child ( 60 ) Recognized Class Woman ( 19 ) 13 4 2 Child ( 63 ) 5 1 57\n",
       "\n",
       "By collapsing the classes \"Woman\" and \"Child\", we can delve deeper into the performance of the example:\n",
       "\n",
       "  - For the class \"Woman,\" we observe a high specificity (correctly dismissing the class) but low precision and sensitivity values. A closer examination of the prevalence also indicates that the high specificity and accuracy values are a consequence of the class imbalance ( 20 / 100 = 20% )\n",
       "\n",
       "  - For the class \"Child,\" we observe high values for sensitivity, specificity, and precision, indicating that the method successfully recognizes children in the images. The balanced prevalence of 60/100=60% and the high accuracy of 91% suggest that the method is performing well for this class Actual Class Total Population Woman ( P=20 ) Not a Woman ( N=80 ) Recognized Class Woman ( 19 ) True Positive ( TP=13 ) False Positive ( FP=6 ) 𝑃𝑃𝑉 = Not a Woman ( 81 ) False Negative ( FN=7 ) True Negative ( TN=74 ) 𝑁𝑃𝑉 = Actual Class Total Population Child ( P=60 ) Not a Child ( N=40 ) Recognized Class Child ( 63 ) True Positive ( TP=57 ) False Positive ( FP=6 ) 𝑃𝑃𝑉 = Not a Child ( 37 ) False Negative ( FN=3 ) True Negative ( TN=34 ) 𝑁𝑃𝑉 = precision precision sensitivity specificity sensitivity specificity\n",
       "\n",
       "## 2.6 Optimizing Hyperparameters\n",
       "\n",
       "In many binary classification algorithms, the output is not a direct assignment to a class, but rather a prediction score. Take the example on the top right, where there is a clear correlation between glucose concentration and the presence of diabetes:\n",
       "\n",
       "  - Plotting the distribution of concentration results in two curves: green for healthy people and red for those with diabetes\n",
       "\n",
       "  - The distributions overlap, causing uncertainty in determining whether a person is healthy or sick in this overlapping area\n",
       "\n",
       "  - To make predictions, we need to choose a threshold, denoted as ∗ in the figure. Scores (glucose concentration in this case) below 𝑥 ∗ are classified as healthy while scores above 𝑥 ∗ are classified as sick. 𝑥 ∗ becomes a hyperparameter\n",
       "\n",
       "But how do we select 𝑥 ∗ ? Let’s consider the bottom right figure:\n",
       "\n",
       "  - Scores below the threshold are predicted as negative cases which we referred to as \"false\" in the confusion matrix\n",
       "\n",
       "  - Scores above the threshold are predicted as positive cases which we referred to as “true\" in the confusion matrix\n",
       "\n",
       "  - In general, the actual distributions of scores in the positive and negative classes overlap, making it impossible to perfectly separate the two classes. This results in false negatives (scores below threshold but actually positive cases) and false positives (scores above threshold but actually negative cases)\n",
       "\n",
       "  - In the example on the top right, we can choose the threshold 𝑥 as shown to safely rule out diabetes with minimal false negatives (high sensitivity). Alternatively, we could set 𝑥 further to the right to reduce false positives and accurately identify people with diabetes (high specificity) source: https://docs.aws.amazon.com/machine - learning/latest/dg/binary - classification.html healthy disease\n",
       "\n",
       "Let’s have a closer look at how the setting of the threshold impacts the results of the prediction:\n",
       "\n",
       "  - The plot on the bottom left shows the actual distribution of scores between 0 and 1 for the negative class as 𝑓 𝑛 ( 𝑥 ) and for the positive class as 𝑓 𝑝 ( 𝑥 ) . When we introduce a threshold 𝑇 , we classify a score 𝑥 < 𝑇 as negative\n",
       "\n",
       "  - By considering only 𝑓 𝑛 ( 𝑥 ) , we can determine the true negatives (green) and false positives (red) as shown in the upper right plot. The true negative rate (TNR), or specificity, is then the integral of 𝑓 𝑛 ( 𝑥 ) over scores from − ∞ to 𝑇 (the green area under 𝑓 𝑛 ( 𝑥 ) ). Similarly, we obtain the false positive rate (FPR) as the integral of 𝑓 𝑛 ( 𝑥 ) over scores from 𝑇 to + ∞ (the red area under 𝑓 𝑛 ( 𝑥 ) )\n",
       "\n",
       "  - By considering only 𝑓 𝑝 ( 𝑥 ) , we can determine the true positives (green) and false negatives (red) as shown in the lower right plot. The false negative rate (FNR) is then the integral of 𝑓 𝑝 ( 𝑥 ) over scores from − ∞ to 𝑇 (the red area under 𝑓 𝑝 ( 𝑥 ) ). Similarly, we obtain the true positive rate (TPR), or sensitivity, as the integral of 𝑓 𝑝 ( 𝑥 ) over scores from 𝑇 to + ∞ (the green area under 𝑓 𝑝 ( 𝑥 ) )\n",
       "\n",
       "  - When the two distributions, 𝑓 𝑛 ( 𝑥 ) and 𝑓 𝑝 ( 𝑥 ) , overlap, shifting the threshold 𝑇 to the left will increase the true positive rate (TPR), or sensitivity, while decreasing the true negative rate (TNR), or specificity. Conversely, moving the threshold 𝑇 to the right will decrease the TPR, or sensitivity, while increasing the TNR, or specificity\n",
       "\n",
       "  - Prevalence can significantly impact the outcome. For example, if we have 10 times more negative cases, having equal values for TNR and TPR would result in 10 times more true negatives than true positives sensitivity specificity\n",
       "\n",
       "The Receiver Operating Characteristic curve , or ROC curve , is a graph that visualizes the performance of a binary classifier by varying the threshold. It plots true positive rate (TPR, sensitivity) on the y - axis, and false positive rate (FPR, 1 - specificity) on the x - axis. We can generate the ROC curve by sorting the data items in the validation set based on their scores in descending order, as illustrated in the table below:\n",
       "\n",
       "  - The first column, labeled \"Class“, represents the actual class of each item (ground truth). The second column, labeled \"Score“, contains the scores assigned by the binary classifier to each item\n",
       "\n",
       "  - The threshold for each row is set to the score value in that row. All items at and above the row's threshold are classified as \"positives\" by the classifier, while all items below the threshold are classified as \"negatives\"\n",
       "\n",
       "  - Let's focus on the highlighted row in the table: using a threshold of 0.54, we can determine the true positives (TP=5, by counting all P’s above and including the row), false positives (FP=1, by counting all N’s above and including the row), false negatives (FN=5, by counting all P’s below the row), and true negatives (TN=9, by counting all N’s below the row). With a total of 10 P’s and N’s, we can calculate the TPR and FPR and then plot the results, as shown in the graph on the right - hand side. Class Score TP FP FN TN TPR FPR ACC 1 - specificity sensitivity Threshold ( T ) for this point (sensitivity) 𝐹𝑃𝑅 (1 - specificity) Ideal point with maximum sensitivity and specificity\n",
       "\n",
       "Interpretation of the ROC - curve: consider the 4 methods A, B, C, D and their confusion matrices below:\n",
       "\n",
       "  - A lies in an are with high sensitivity. If the 𝑁𝑃𝑉 is also high (consider the prevalence), then A can effectively “rule out” (negative predictions). A would be a good candidate for the diagnostic process to rule out diseases\n",
       "\n",
       "  - B lies in the area below the diagonal. In such cases, we can construct B' which negates the outcome of B to obtain a better method: 𝑇𝑃 and FN switch their values; 𝐹𝑃 and 𝑇𝑁 switch their values. This results in new values for the diagram as follows: 𝑇𝑃𝑅′ = 1 − 𝑇𝑃𝑅 = 60% and 𝐹𝑃𝑅′ = 1 − 𝐹𝑃𝑅 = 20%\n",
       "\n",
       "  - C has a high sensitivity but a lower 𝑁𝑃𝑉 than A due to its lower specificity. This affects its ability to \"rule out,\" and a negative prediction is incorrect in 25% of the cases, making it unsuitable for many scenarios\n",
       "\n",
       "  - D lies in an area with high specificity. If the 𝑃𝑃𝑉 is also high (consider the prevalence), then D can effectively “rule in” (positive predictions). D would be a good candidate to provide evidence for the presence of a disease\n",
       "\n",
       "  - Finally, note that the points 𝑇𝑃𝑅 = 0 , 𝐹𝑃𝑅 = 0 and ( 𝑇𝑃𝑅 = 1 , 𝐹𝑃𝑅 = 1 ) are the results of extreme thresholds that always return negative or positive predictions, respectively. Methods close to these areas, including C below, may exhibit a too strong bias towards negative or positive predictions (recall) A B C D 0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100% TPR (recall) FPR (fall-out) better worse High sensitivity → a bility to “rule out” (neg, prediction) if NPV is high High specificity → ability to “rule in” (pos. prediction) if PPV is high (sensitivity) 𝐹𝑃𝑅 (1 - specificity)\n",
       "\n",
       "Back to the example from before, depicted again the bottom of the page:\n",
       "\n",
       "  - To find the optimal threshold, we must consider the specific performance goal for our task. If we aim to \"rule out\" or \"rule in\" the condition of the task, we choose thresholds with corresponding 𝑇𝑃𝑅 and 𝐹𝑃𝑅 values in or close to that area as highlighted on the previous page\n",
       "\n",
       "  - In machine learning classification tasks, accuracy is a commonly used performance measure. In this case, we would select the threshold that maximizes the accuracy value. In the example shown below, the threshold of 0.54 gives the highest accuracy and is thus a good choice for this scenario\n",
       "\n",
       "  - If we don't have a specific performance goal, we can choose the threshold that is closest to the ideal point in the upper left corner. Alternatively, we can optimize for the sum of sensitivity and specificity to make the decision\n",
       "\n",
       "  - The area under the ROC curve (AUC) is a comprehensive performance measure across all thresholds. It reflects how well a method can distinguish between positive and negative predictions based on the computed scores. In the example below, if a method consistently assigns higher scores to the P's than the N's, the AUC would cover the entire space Class Score TP FP FN TN TPR FPR ACC 1 - specificity sensitivity Threshold ( T ) for this point (sensitivity) 𝐹𝑃𝑅 (1 - specificity) Ideal point with maximum sensitivity and specificity\n",
       "\n",
       "## 2.7 Literature and Links\n",
       "\n",
       "MS MARCO (Microsoft Machine Reading Comprehension Dataset), https://microsoft.github.io/msmarco/\n",
       "\n",
       "Text REtrieval Conference , http://trec.nist.gov/\n",
       "\n",
       "Kaggle Competitions , https://www.kaggle.com/competitions\n",
       "\n",
       "Kent, Allen; Berry, Madeline M.; Luehrs , Jr., Fred U.; Perry, J.W. (1955). Machine literature searching VIII. Operational criteria for designing information retrieval systems . American Documentation. 6 (2): 93. doi : 10.1002/asi.5090060209 .\n",
       "\n",
       "Baeza - Yates, Ricardo; Ribeiro - Neto, Berthier (1999). Modern Information Retrieval . New York, NY: ACM Press, Addison - Wesley. ISBN 0 - 201 - 39829 - X\n",
       "\n",
       "David A. Grossman, Ophir Frieder, Information Retrieval Algorithms and Heuristics , Kluwer Academic Publishers, 1998\n",
       "\n",
       "Zou, Kelly H.; O'Malley, A. James; Mauri, Laura (2007); Receiver - operating characteristic analysis for evaluating diagnostic tests and predictive models , Circulation, 115(5):654 – 7, http://circ.ahajournals.org/content/115/5/654.full\n",
       "\n",
       "Hanley, James A.; McNeil, Barbara J. (1982). The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve, Radiology . 143 (1): 29 - 36. PMID 7063747 . doi : 10.1148/radiology.143.1.7063747 .\n",
       "\n",
       "Fawcett, Tom (2006), An Introduction to ROC Analysis , Pattern Recognition Letters . 27 (8): 861 – 874. doi : 10.1016/j.patrec.2005.10.010 or http://people.inf.elte.hu/kiss/11dwhdm/roc.pdf\n",
       "\n",
       "Amazon AWS, Amazon Machine Learning – Developer Guide , https://docs.aws.amazon.com/machine - learning/latest/dg/evaluating - model - accuracy.html\n",
       "\n",
       "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze , Introduction to Information Retrieval, Chapter 8: Evaluation in information retrieval, Cambridge University Press. 2008. Online version: https://nlp.stanford.edu/IR - book/\n",
       "\n",
       "D. R. Radev ; H. Qi; H. Wu; W. Fan (2002). Evaluating web - based question answering systems, Proceedings of LREC. http://www.lrec - conf.org/proceedings/lrec2002/pdf/301.pdf\n",
       "\n",
       "Related Wikipedia articles\n",
       "\n",
       "  - Evaluation measures , https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
       "\n",
       "  - Discounted cumulative gain , https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
       "\n",
       "  - Confusion Matrix , https://en.wikipedia.org/wiki/Confusion_matrix Note that the Wikipedia article uses the transposed confusion matrix. There is also some comments on that on the discussion p age\n",
       "\n",
       "  - ROC curve , https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "pages = flatten(map(extract_parts, reader.pages))\n",
    "display(Markdown(\"\\n\\n\".join(pages)))\n",
    "list(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f18b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
