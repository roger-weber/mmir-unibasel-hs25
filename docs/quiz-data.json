{"meta-data": {"title": "Multimedia Retrieval HS25", "description": "Exercise questions for MMIR HS25, University Basel"}, "questions": {"01 - Introduction": [{"question": "Which key technological shift allowed large-scale information retrieval to emerge in the 1970s?", "options": ["Development of GPU-based parallel computing platforms", "Rise of computer typesetting and word processing creating machine-readable text", "Introduction of transformer architectures", "Creation of cloud-based storage services"], "correct": 1, "explanation": "The 1970s saw widespread computer typesetting and word processing, producing machine-readable text that enabled large-scale retrieval systems."}, {"question": "What major innovation distinguished Google's PageRank from earlier web search algorithms?", "options": ["It indexed only multimedia content instead of text.", "It ranked pages by the authority of inbound links rather than just keyword matching.", "It eliminated the need for any ranking and returned results in random order.", "It introduced manual tagging for every webpage."], "correct": 1, "explanation": "PageRank ranked pages based on the authority of links pointing to them, moving beyond pure keyword matching."}, {"question": "Why is the 'semantic gap' particularly challenging in multimedia retrieval?", "options": ["Because low-level features like pixels or frequencies do not directly represent human-level concepts.", "Because text retrieval research is more established and advanced.", "Because storage devices cannot handle multimedia file sizes.", "Because queries in multimedia retrieval are slower to process."], "correct": 0, "explanation": "The semantic gap refers to the mismatch between low-level media features and high-level human semantics, making direct matching difficult."}, {"question": "Which retrieval architecture first combines a retriever with a language model to generate a full answer rather than extracting a single passage?", "options": ["Retriever-Reader", "Retriever-Generator (RAG)", "Retriever-Filter", "Generator-only"], "correct": 1, "explanation": "Retriever-Generator, also known as Retrieval-Augmented Generation (RAG), combines a retriever with a generative model to produce comprehensive answers."}, {"question": "Suppose you need to design a retrieval system that can infer user interests without explicit queries, as in TikTok recommendations. Which relevance strategy is most appropriate?", "options": ["Objective relevance using keyword matching", "Manual tagging and classification of all content", "Implicit relevance estimation from behavioral signals and context", "Only inverse document frequency scoring"], "correct": 2, "explanation": "Query-less search relies on behavioral signals and contextual data to infer user intent and estimate relevance without explicit queries."}, {"question": "Given the fastest PCI-E 5.0 NVMe SSD read rate of about 14,000 MB/s, approximately how long would it take to sequentially read 1 petabyte of data?", "options": ["Around 20 hours", "Around 1 day", "Around 8 days", "Around 1 year"], "correct": 2, "explanation": "1 petabyte is about 1,000,000,000 MB; at 14,000 MB/s it would take roughly 71,000 seconds (~20 hours) per 1 TB, so about 8 days for 1 PB."}, {"question": "Which retrieval architecture simply returns documents matching a query without explicit ranking?", "options": ["Retriever-Only", "Retriever-Ranker", "Retriever-Reader", "Retriever-Generator"], "correct": 0, "explanation": "Retriever-only systems fetch and present matching documents directly without filtering or ranking stages."}, {"question": "Why is searching images inherently more complex than searching text, according to the concept of the semantic gap?", "options": ["Images require more storage space than text", "Pixels have no fixed, direct relation to high-level concepts", "Image queries always need manual tagging", "Text search engines cannot process metadata"], "correct": 1, "explanation": "The semantic gap is the mismatch between low-level image features and the human semantics they represent."}, {"question": "Which statement best describes Retrieval-Augmented Generation (RAG)?", "options": ["It relies solely on a language model without external information", "It retrieves relevant documents and combines them with a query for a generative model to answer", "It uses Boolean logic to refine search results", "It is limited to ranking web pages by link authority"], "correct": 1, "explanation": "RAG retrieves supporting documents and feeds them with the query into a language model to generate a comprehensive answer."}, {"question": "The exponential data growth described in the text implies which critical challenge for retrieval systems?", "options": ["Lower hardware costs make retrieval trivial", "Indexing and ranking must occur nearly in real time to remain relevant", "Users no longer require high recall", "Query-less search becomes impossible"], "correct": 1, "explanation": "Rapidly created data such as news and social posts require near real-time indexing and ranking to stay relevant."}], "02 - Classical Retrieval": [{"question": "In the classical Boolean retrieval model, how is a document's relevance to a query determined?", "options": ["By calculating the cosine similarity between document and query vectors", "By checking if the document satisfies the Boolean expression of the query", "By estimating the probability that the document is relevant", "By counting the frequency of each query term"], "correct": 1, "explanation": "The Boolean model includes a document if it satisfies the Boolean query expression, without using ranking or probability."}, {"question": "Which representation records how often each term appears but ignores the order of terms?", "options": ["Set-of-words model", "Bag-of-words model", "Vector embedding model", "Probabilistic model"], "correct": 1, "explanation": "The bag-of-words model preserves term frequencies but discards word order and proximity."}, {"question": "What is a primary motivation for chunking long documents into smaller units before indexing?", "options": ["To reduce the size of the vocabulary", "To eliminate the need for metadata extraction", "To improve retrieval precision by ensuring query terms occur closer together", "To avoid the need for an inverted index"], "correct": 2, "explanation": "Chunking forces query terms to appear within the same segment, which improves precision by reducing false positives due to distant term occurrences."}, {"question": "Which feature distinguishes faceted search from basic Boolean retrieval?", "options": ["Faceted search requires all query terms to be present", "Users can add or remove filters and sort results without resubmitting the search", "It relies solely on probabilistic ranking of documents", "It eliminates the need for metadata attributes"], "correct": 1, "explanation": "Faceted search allows interactive filtering and sorting independent of the original query, improving navigation of large result sets."}, {"question": "Why are stop words like \"the\" often given very low weight in modern retrieval systems?", "options": ["They rarely appear in documents", "They significantly increase the size of the index", "They have high document frequency and low discriminative power", "They cannot be tokenized correctly"], "correct": 2, "explanation": "Stop words appear in most documents and thus cannot differentiate relevant from non-relevant documents, reducing their discriminative power."}, {"question": "In the indexing pipeline's offline phase, which step directly produces high-dimensional feature vectors for retrieval?", "options": ["Tokenization", "Summarization", "Metadata extraction", "Chunk splitting"], "correct": 1, "explanation": "During summarization, tokens are transformed into high-dimensional feature representations used for indexing and later retrieval."}, {"question": "The discrimination power dp(t) of a term t, as defined by Salton, Wong, and Yang (1975), is high when:", "options": ["Removing t causes documents to become more similar to the collection centroid", "Term t occurs in almost all documents", "Term t has low inverse document frequency", "The similarity to the centroid decreases upon removing t"], "correct": 0, "explanation": "A high dp(t) means that removing the term increases the similarity of documents to the centroid, showing that the term previously helped to distinguish documents."}, {"question": "Which key property makes text retrieval less affected by the semantic gap compared to other media?", "options": ["It always uses structured metadata", "It directly matches user text queries to terms in unstructured documents", "It relies on image recognition algorithms", "It requires manual annotation of every document"], "correct": 1, "explanation": "Text retrieval matches user-entered text queries directly to terms in unstructured documents, minimizing issues caused by semantic interpretation."}, {"question": "What is the main advantage of Boolean retrieval systems when scanning data?", "options": ["They can score and rank results in real time", "They determine each document's relevance independently without post-processing", "They automatically expand queries with synonyms", "They always outperform vector-space models"], "correct": 1, "explanation": "Boolean retrieval determines relevance for each document independently and does not require a post-processing ranking step."}, {"question": "In the set-of-words model, how is a document represented?", "options": ["As a sequence of characters with full formatting", "As a set of unique terms, ignoring order and frequency", "As a weighted graph of co-occurring phrases", "As a probability distribution of word embeddings"], "correct": 1, "explanation": "The set-of-words model records only the presence of unique terms, ignoring both order and frequency."}, {"question": "Why was faceted search introduced as an extension to the Boolean model?", "options": ["To enable partial matches of query terms", "To improve tokenization of complex documents", "To allow users to filter and sort large result sets interactively", "To compute inverse document frequency scores"], "correct": 2, "explanation": "Faceted search was added to help users navigate large result sets by applying filters and sorting without changing document relevance."}, {"question": "What key improvement did the Extended Boolean Model introduce over the classical Boolean Model?", "options": ["Support for natural language queries", "Partial matching and term-frequency-based relevance scoring", "Integration of hyperlink analysis", "Automatic language translation"], "correct": 1, "explanation": "The Extended Boolean Model allows partial matches and uses term frequency to assign relevance scores, enabling ranked results."}, {"question": "In the offline indexing phase, what is the primary purpose of the 'split' step?", "options": ["To tokenize text into words and subwords", "To divide long documents into smaller coherent retrieval units", "To normalize character encodings to UTF-8", "To rank documents by retrieval status value"], "correct": 1, "explanation": "During the split step, long documents are divided into smaller searchable chunks, enabling finer-grained retrieval."}, {"question": "Why is overlapping text sometimes added when chunking long documents?", "options": ["To improve storage compression", "To avoid false negatives when query terms span adjacent chunks", "To simplify vector-space calculations", "To increase the inverse document frequency of rare terms"], "correct": 1, "explanation": "Overlapping text ensures that query terms at chunk boundaries are captured together, reducing false negatives."}, {"question": "Which factor best explains why terms with very low or very high document frequency have low discrimination power?", "options": ["They cannot be tokenized properly", "They fail to differentiate documents effectively", "They have high inverse document frequency", "They are always removed as stop words"], "correct": 1, "explanation": "Terms appearing in too few or too many documents provide little ability to distinguish relevant from non-relevant documents."}, {"question": "Which key limitation of the Standard Boolean Model motivated the development of ranked retrieval methods?", "options": ["Lack of partial match capability", "Excessive computational complexity", "Dependence on term frequencies", "Requirement of probabilistic feedback"], "correct": 0, "explanation": "The Standard Boolean Model returns only documents that fully satisfy the Boolean query and cannot rank or partially match documents, leading to either too many or too few results."}, {"question": "Why does the cosine similarity measure reduce the bias toward longer documents compared to the inner product?", "options": ["It ignores inverse document frequency", "It normalizes vectors by their length", "It uses only binary term presence", "It excludes query term frequencies"], "correct": 1, "explanation": "Cosine similarity divides by the product of vector lengths, so document length does not directly inflate similarity scores."}, {"question": "Which assumption is fundamental to the Binary Independence Model?", "options": ["Documents are represented as dense real-valued vectors", "Term independence and binary term presence", "Relevance depends on query term proximity", "Term frequencies must be normalized"], "correct": 1, "explanation": "The BIR assumes terms occur independently and represent documents as binary (present/absent) sets, disregarding exact frequencies."}, {"question": "How does BM25 address the issue of very frequent query terms dominating the score?", "options": ["By discarding all terms appearing in more than 50% of documents", "By applying a saturation function to term frequency", "By using only binary term weights", "By removing inverse document frequency entirely"], "correct": 1, "explanation": "BM25 uses a nonlinear saturation of term frequency controlled by parameter k, preventing excessive influence of very frequent terms."}, {"question": "What role does the parameter b play in the BM25 scoring formula?", "options": ["Controls how document length normalizes term frequency", "Determines the inverse document frequency smoothing", "Sets the minimum similarity threshold for retrieval", "Specifies the user feedback weight in relevance estimation"], "correct": 0, "explanation": "The b parameter adjusts the impact of document length on term frequency saturation, favoring shorter or longer documents depending on its value."}, {"question": "A researcher wants a retrieval model that (1) provides a probabilistic foundation, (2) allows partial matches, (3) incorporates term frequency with diminishing returns, and (4) accounts for document length. Which model best satisfies all these requirements?", "options": ["Standard Boolean Model", "Vector Space Model with cosine similarity", "Binary Independence Model", "BM25"], "correct": 3, "explanation": "BM25 integrates probabilistic principles, supports partial matching, applies saturated term frequency, and normalizes by document length, fulfilling all four criteria."}, {"question": "Which statement best describes the primary limitation of the Standard Boolean Model in information retrieval?", "options": ["It requires complex probabilistic calculations.", "It cannot rank documents by relevance.", "It depends on user feedback to estimate probabilities.", "It only works with normalized vectors."], "correct": 1, "explanation": "The Standard Boolean Model only filters documents as relevant or not without ranking them by relevance."}, {"question": "What key advantage does the Vector Space Model have over the Standard Boolean Model?", "options": ["It guarantees independent term assumptions.", "It provides ranked retrieval based on similarity measures.", "It avoids using term frequencies entirely.", "It uses only Boolean logic for query processing."], "correct": 1, "explanation": "The Vector Space Model ranks documents by computing similarity between query and document vectors."}, {"question": "In the Vector Space Model, which measure computes the angle between the query and document vectors?", "options": ["Inner product", "Bayes' theorem", "Cosine similarity", "P-norm model"], "correct": 2, "explanation": "The cosine measure calculates the angle between query and document vectors to assess similarity."}, {"question": "According to the Binary Independence Model, which assumption is made about terms absent from the query?", "options": ["They are given higher weights.", "They do not affect document ranking.", "They are normalized using idf values.", "They must be present in all relevant documents."], "correct": 1, "explanation": "The BIR model assumes that terms absent from the query are equally distributed in relevant and non-relevant documents and thus do not affect ranking."}, {"question": "Which feature distinguishes the Extended Boolean Model from the Vector Space Model despite both supporting ranked retrieval?", "options": ["The Extended Boolean Model uses Boolean query structure while Vector Space treats queries as vectors.", "The Extended Boolean Model uses cosine similarity while Vector Space does not.", "The Extended Boolean Model ignores idf weighting while Vector Space requires it.", "The Extended Boolean Model relies on Bayes' theorem for probability estimation."], "correct": 0, "explanation": "Extended Boolean retains Boolean expressions for queries, unlike the Vector Space Model which treats queries as weighted vectors."}, {"question": "Consider a retrieval scenario with very frequent terms appearing in more than 50% of documents. According to the BM25 derivation from the BIR model, what happens to the idf-like weight c_j for such terms and why is this significant for ranking?", "options": ["c_j becomes zero, ensuring these common terms have no influence on ranking.", "c_j becomes negative, potentially lowering the score contribution of very common terms.", "c_j grows without bound, dominating the score.", "c_j remains constant, giving frequent terms equal weight to rare terms."], "correct": 1, "explanation": "For very frequent terms, the BM25/BIR-derived c_j can be negative, reducing their contribution and preventing common terms from dominating the ranking."}, {"question": "In the Binary Independence Model, why can terms absent from the query be ignored in ranking?", "options": ["They have zero inverse document frequency", "Their occurrence probability is assumed equal in relevant and non-relevant documents", "They are treated as stop words by default", "They cancel out during cosine normalization"], "correct": 1, "explanation": "BIR assumes terms not in the query have the same probability in relevant and non-relevant documents, so they do not affect ranking."}, {"question": "Suppose a corpus contains extremely long documents where relevant terms are scattered far apart. Which combination of strategies best improves both precision and recall without excessive index growth?", "options": ["Use only fixed-size chunking without overlap", "Apply structural chunking with overlapping windows", "Rely solely on term stemming", "Increase BM25's k parameter and ignore chunking"], "correct": 1, "explanation": "Structural chunking preserves semantic boundaries while overlapping windows reduce false negatives, balancing precision and recall for long documents."}, {"question": "Compare how the Extended Boolean Model, Vector Space Model, and BM25 each handle term frequency when ranking documents. Which statement best describes their key differences?", "options": ["All three ignore term frequency entirely, focusing only on binary presence of terms.", "Extended Boolean uses term frequency to assign similarity scores, Vector Space weights terms by tf-idf without saturation, and BM25 adds frequency saturation and length normalization.", "Extended Boolean and Vector Space use tf-idf with saturation, while BM25 ignores term frequency.", "Vector Space and BM25 both ignore term frequency but normalize for document length."], "correct": 1, "explanation": "Extended Boolean incorporates term frequency for partial matches, Vector Space uses tf-idf without saturation, and BM25 introduces frequency saturation with document length normalization."}, {"question": "Which best describes the relationship between the offline indexing pipeline and linguistic preprocessing like stemming or lemmatization?", "options": ["Stemming is performed after the index is built to reduce vector dimensionality.", "Tokens are stemmed or lemmatized during feature extraction so the index stores normalized terms for matching queries.", "Lemmatization is only applied at query time and never during indexing.", "The inverted index itself performs stemming dynamically for each query."], "correct": 1, "explanation": "During the offline phase, stemming or lemmatization occurs in the tokenization/feature extraction stage so the index uses normalized terms."}, {"question": "How does user relevance feedback in the Binary Independence Model (BIR) relate to BM25's scoring approach?", "options": ["Feedback adjusts BM25's k and b hyperparameters directly.", "BIR uses feedback to refine probabilities r_j and n_j, similar to how BM25 adjusts IDF weights but without direct feedback loops.", "BM25 requires feedback to estimate IDF values just like BIR.", "Both models ignore user feedback entirely."], "correct": 1, "explanation": "BIR iteratively updates r_j and n_j using feedback, conceptually similar to BM25's probabilistic weighting though BM25 does not require explicit feedback."}, {"question": "When incorporating HTML metadata and anchor text into a BM25-based retrieval engine, which weighting strategy is most appropriate to counteract spammy anchor texts?", "options": ["Assign extremely high weights to all anchor text terms to ensure recall.", "Treat anchor text terms with the same weight as body text regardless of context.", "Index anchor text but down-weight overly common or misleading phrases to limit click-bait influence.", "Exclude all anchor text from the index to avoid bias."], "correct": 2, "explanation": "Anchor text is valuable but can be manipulated; indexing with cautious down-weighting mitigates spam without losing relevance signals."}, {"question": "How can Zipf's law guide stop-word removal and IDF weighting for a multilingual corpus to improve both Boolean and Vector Space retrieval?", "options": ["By enforcing equal frequency of all terms across languages.", "By identifying extremely high- and low-frequency words for potential down-weighting or exclusion, refining IDF weighting.", "By ensuring rare terms are always removed to reduce vocabulary size.", "By ranking terms purely by alphabetical order to maintain neutrality."], "correct": 1, "explanation": "Zipf's law highlights very frequent and very rare terms, informing stop-word lists and IDF weighting to balance discrimination and recall."}], "03 - Evaluation": [{"question": "What is the primary purpose of benchmarks in information retrieval?", "options": ["To provide anecdotal evidence of a system's performance", "To allow consistent comparison across different retrieval methods", "To replace the need for relevance judgments", "To rank documents manually by users"], "correct": 1, "explanation": "Benchmarks provide a shared test environment that enables fair and reproducible comparisons across different retrieval methods."}, {"question": "Which of the following is NOT typically a component of an effective benchmark?", "options": ["Document collection", "Set of queries", "Relevance judgments", "User interface design"], "correct": 3, "explanation": "An effective benchmark consists of a document collection, queries, relevance judgments, and performance goals, but not the system's user interface."}, {"question": "In legal document discovery, which metric is most important?", "options": ["Top-10 precision", "High recall", "Low latency", "Normalized Discounted Cumulative Gain (nDCG)"], "correct": 1, "explanation": "Legal discovery requires retrieving nearly all relevant documents, making recall the critical metric."}, {"question": "What is the main difference between macro and micro evaluation of retrieval systems?", "options": ["Macro averages per-query metrics equally; micro aggregates counts across all queries before computing metrics.", "Macro evaluates only top-ranked documents; micro evaluates all documents.", "Macro focuses on binary relevance; micro uses graded relevance.", "Macro is used for web search; micro is used for legal discovery."], "correct": 0, "explanation": "Macro evaluation gives equal weight to each query, while micro evaluation sums counts across queries and then computes precision/recall."}, {"question": "Why is the Mean Reciprocal Rank (MRR) particularly useful for sparse relevance assessments?", "options": ["It focuses on the first relevant document rather than all retrieved documents", "It measures overall recall more accurately", "It ignores ranking and only considers document presence", "It replaces the need for precision-recall curves"], "correct": 0, "explanation": "MRR focuses on the rank of the first relevant document, making it robust when many documents remain unjudged in sparse assessments."}, {"question": "Which retrieval architecture is primarily responsible for improving top-ranked precision without user intervention?", "options": ["Retriever only", "Retriever-Filter", "Retriever-Ranker", "Boolean retrieval"], "correct": 2, "explanation": "Retriever-Ranker systems assign relevance scores to candidates, improving precision at the top of the ranked list automatically."}, {"question": "How does Normalized Discounted Cumulative Gain (nDCG) differ from simple Precision@k?", "options": ["nDCG penalizes lower-ranked relevant documents and accounts for graded relevance", "nDCG only considers binary relevance and ignores ranking", "Precision@k considers all retrieved documents, while nDCG considers only the first document", "nDCG cannot handle top-k evaluations"], "correct": 0, "explanation": "nDCG incorporates both the position of relevant documents and graded relevance, while Precision@k only counts relevant documents without ranking weights."}, {"question": "A retrieval system returns 75% precision and 45% recall for a query with 20 relevant documents. If the goal is exhaustive research, which F-\u00ce\u00b2 value would emphasize recall over precision, and why?", "options": ["\u00ce\u00b2=0.5, because it prioritizes precision for fact-checking", "\u00ce\u00b2=1, because it balances precision and recall equally", "\u00ce\u00b2=2, because it emphasizes recall for exhaustive retrieval", "\u00ce\u00b2=\u00e2\u02c6\u017e, because it ignores both precision and recall"], "correct": 2, "explanation": "Setting \u00ce\u00b2>1 (e.g., \u00ce\u00b2=2) in the F-measure emphasizes recall over precision, which is suitable for exhaustive research where retrieving all relevant documents is critical."}, {"question": "In the context of information retrieval, precision measures:", "options": ["The total number of relevant documents in the collection", "The proportion of retrieved documents that are relevant", "The speed at which a system returns results", "The percentage of relevant documents not retrieved"], "correct": 1, "explanation": "Precision measures how many of the retrieved documents are relevant from the user's perspective."}, {"question": "In the context of information retrieval, recall measures:", "options": ["The total number of documents retrieved", "The proportion of relevant documents that are retrieved", "The speed at which results are returned", "The rank of the first relevant document"], "correct": 1, "explanation": "Recall measures the percentage of relevant documents retrieved out of all relevant documents in the collection."}, {"question": "What does a precision-recall curve illustrate in information retrieval?", "options": ["The trade-off between precision and recall across different thresholds", "The system's response time over multiple queries", "The cumulative gain of the top-ranked document only", "The total number of queries evaluated"], "correct": 0, "explanation": "A precision-recall curve shows how precision and recall change as more documents are retrieved, illustrating their trade-off."}, {"question": "System efficiency in information retrieval can be assessed by:", "options": ["Measuring the distance of the PR-curve to the ideal point", "Counting only the total number of retrieved documents", "Calculating micro-averaged recall alone", "Evaluating relevance grades without considering ranking"], "correct": 0, "explanation": "System efficiency is measured by the distance of the precision-recall curve to the ideal point, combining precision and recall performance into a single metric."}, {"question": "Which scenario would prioritize high recall over precision?", "options": ["Fact-checking a single query quickly", "Legal document discovery for a lawsuit", "Recommending a few products to a user", "Searching for a quick answer on a news website"], "correct": 1, "explanation": "In legal discovery, finding all relevant documents is critical, so recall is prioritized over precision."}, {"question": "Why might micro evaluation be preferred over macro evaluation in some retrieval tasks?", "options": ["It emphasizes worst-case performance across all queries", "It gives equal weight to queries with few relevant documents", "It aggregates all true positives and retrieved documents, reducing the impact of small queries", "It requires fewer relevance judgments"], "correct": 2, "explanation": "Micro evaluation sums TP and retrieved/relevant documents before calculating metrics, making it less sensitive to small queries."}, {"question": "What does Mean Reciprocal Rank (MRR) specifically measure?", "options": ["The precision of all retrieved documents", "The average rank of the first relevant document across queries", "The recall of top-k documents for each query", "The normalized cumulative gain of the top results"], "correct": 1, "explanation": "MRR focuses on the rank of the first relevant document, averaging its reciprocal over all queries."}, {"question": "Which metric accounts for graded relevance and the position of a document in the ranking?", "options": ["Precision at k (P@k)", "Recall", "Discounted Cumulative Gain (DCG)", "F1-score"], "correct": 2, "explanation": "DCG considers both the graded relevance of documents and their rank, penalizing relevant documents in lower positions."}, {"question": "In a Retriever-Ranker architecture, how is precision mainly improved?", "options": ["By expanding the document collection", "By giving relevance scores to candidates and ranking them", "By applying user filters like date or rating", "By increasing throughput and reducing latency"], "correct": 1, "explanation": "The ranker assigns scores to retrieved documents, improving precision at the top of the ranked list."}, {"question": "In a fact-checking scenario, which metric is usually most important?", "options": ["High recall across all documents", "Top results precision", "Throughput of queries", "Normalized DCG over all results"], "correct": 1, "explanation": "Fact-checking prioritizes quickly finding mostly relevant documents, making top results precision critical."}, {"question": "What distinguishes dense from sparse relevance assessments in large-scale retrieval benchmarks?", "options": ["Dense assessments judge only the top results; sparse assess all documents", "Dense assessments involve human assessors; sparse assessments are automated", "Dense assessments cover most retrieved documents; sparse assess only a subset per query", "Dense assessments ignore relevance; sparse focus on relevance"], "correct": 2, "explanation": "Dense assessments cover most retrieved documents via pooling, while sparse assessments evaluate only subsets due to scale."}, {"question": "In graded relevance assessments, what is the purpose of normalized DCG (nDCG)?", "options": ["To calculate only binary relevance", "To normalize DCG values for comparison across queries", "To evaluate the top result only", "To replace precision and recall entirely"], "correct": 1, "explanation": "nDCG normalizes the DCG value by the ideal ranking, allowing comparisons across queries with different relevance distributions."}, {"question": "Why is the F-Measure useful in retrieval evaluation?", "options": ["It only considers recall", "It only considers precision", "It combines precision and recall into a single metric, adjustable via \u00ce\u00b2", "It measures query latency"], "correct": 2, "explanation": "The F-Measure balances precision and recall with a \u00ce\u00b2 parameter controlling their relative importance."}, {"question": "Which scenario would most benefit from using Mean Reciprocal Rank (MRR) instead of precision-recall metrics?", "options": ["Legal discovery requiring high overall recall", "Fact-checking tasks with sparse relevance assessments", "Biomedical literature search over hundreds of results", "E-commerce recommendations focusing on diversity"], "correct": 1, "explanation": "MRR emphasizes the rank of the first relevant document, making it ideal for fact-checking tasks with sparse judgments."}, {"question": "Given a benchmark with highly variable query sizes, which evaluation approach minimizes the distortion caused by queries with few relevant documents?", "options": ["Macro evaluation", "Micro evaluation", "Mean Reciprocal Rank", "Precision at k"], "correct": 1, "explanation": "Micro evaluation aggregates true positives and totals across all queries, reducing the impact of queries with very few relevant documents."}, {"question": "In information retrieval system design, why might high precision conflict with operational metrics like latency, cost, or scalability?", "options": ["Because optimizing precision always reduces recall", "Because retrieving relevant documents with high precision may require more computational resources", "Because precision and recall are unrelated to user satisfaction", "Because operational metrics are only relevant for offline experiments"], "correct": 1, "explanation": "Maximizing precision often requires retrieving and processing more documents, which can increase computational cost, slow responses, and reduce scalability."}, {"question": "Which approach can be used to automate relevance assessment in large-scale benchmarks while maintaining reliability?", "options": ["Ignoring human judgments entirely and relying only on raw retrieval counts", "Using Large Language Models (LLMs) to suggest initial relevance judgments for human review", "Pooling all documents without applying any sampling or prioritization", "Calculating only precision at k without considering relevance"], "correct": 1, "explanation": "LLMs can generate initial relevance judgments, which humans then review for uncertain cases, improving efficiency while keeping assessments reliable."}], "04 - Advanced Text Processing": [{"question": "Which early method in information retrieval focused on reducing words to their base forms to match queries and documents?", "options": ["Stemming", "Word embeddings", "Transformer models", "N-gram analysis"], "correct": 0, "explanation": "Stemming reduces words to their root forms, enabling simpler matching between queries and documents."}, {"question": "What is the main purpose of query expansion using synonyms in traditional NLP retrieval?", "options": ["To increase recall", "To improve precision", "To reduce computational load", "To enforce exact keyword matching"], "correct": 0, "explanation": "Expanding a query with synonyms increases recall by retrieving more relevant documents, though it may reduce precision."}, {"question": "Why did statistical NLP models like HMMs and n-grams struggle with meaning and long-range dependencies?", "options": ["They relied heavily on local patterns and limited feature engineering", "They used deep neural networks with too many parameters", "They employed symbolic rules instead of data", "They could not process large corpora"], "correct": 0, "explanation": "Statistical models primarily captured local patterns, making them inadequate for understanding long-range dependencies and semantic meaning."}, {"question": "Which advance in the Neural Era allowed models to process sequences in parallel while capturing long-range dependencies?", "options": ["Transformer architecture", "Hidden Markov Models", "ELIZA pattern matching", "N-gram models"], "correct": 0, "explanation": "Transformers use self-attention to process sequences in parallel and capture long-range dependencies, overcoming RNN limitations."}, {"question": "Why are one-hot vectors impractical for large vocabularies in machine learning models?", "options": ["They require large input layers and are sparse", "They cannot encode numerical data", "They lose word order information", "They prevent tokenization of special characters"], "correct": 0, "explanation": "One-hot vectors scale poorly with large vocabularies due to high dimensionality and sparsity, making them inefficient for large models."}, {"question": "How does PMI (Pointwise Mutual Information) prefer bi-grams in a corpus?", "options": ["It favors bi-grams where the terms occur exclusively together and infrequently", "It favors bi-grams with frequent stop words", "It ignores term co-occurrence and uses only frequency", "It ranks bi-grams randomly"], "correct": 0, "explanation": "PMI scores are highest for rare bi-grams where terms occur together exclusively, highlighting meaningful associations."}, {"question": "How do sub-word tokenizations help retrieval systems handle misspellings or non-native pronunciations?", "options": ["By breaking words or phonemes into overlapping sequences, enabling partial matches", "By mapping each token to a unique ID", "By using one-hot vectors for exact matches", "By ignoring infrequent words entirely"], "correct": 0, "explanation": "Overlapping sub-word or phoneme sequences allow retrieval systems to match parts of words even when full words differ due to spelling errors or pronunciation."}, {"question": "In constructing a vocabulary using n-grams and PMI, why might a highly frequent name like 'Sherlock Holmes' appear lower in the PMI ranking but higher using LHR?", "options": ["PMI favors rare co-occurrences, while LHR measures statistical dependence accounting for frequency", "PMI ignores term frequency, while LHR ignores co-occurrence", "PMI and LHR always rank the same bi-grams", "LHR uses semantic embeddings whereas PMI does not"], "correct": 0, "explanation": "PMI favors rare, exclusive co-occurrences, so frequent names can score lower; LHR evaluates dependence between terms, highlighting frequent, significant bi-grams."}, {"question": "What is the role of the '##' prefix in WordPiece tokenization?", "options": ["Indicates a token is at the beginning of a word", "Indicates a token is part of a word's middle or continuation", "Marks tokens as rare vocabulary items", "Signals that a token is a special symbol"], "correct": 1, "explanation": "In WordPiece, '##' denotes subword tokens that continue within a word, helping the tokenizer capture prefixes and shared semantics."}, {"question": "In the LHR method for scoring bi-grams, why might frequent stop-word pairs appear significant?", "options": ["LHR directly counts term frequency without normalization", "LHR compares independence vs. dependence of terms, so dependence can make frequent pairs significant", "Stop words are given higher weights in LHR by default", "PMI filtering is applied before computing LHR"], "correct": 1, "explanation": "LHR measures whether the occurrence of one term depends on another. Even frequent stop words can appear significant if their co-occurrence is highly dependent."}, {"question": "How does Byte Pair Encoding (BPE) handle unseen words in a large corpus?", "options": ["By mapping all unseen words to [UNK]", "By decomposing them into smaller byte-level subwords using the existing vocabulary", "By ignoring them during encoding", "By treating each word as a separate token regardless of frequency"], "correct": 1, "explanation": "BPE can represent previously unseen words by breaking them into smaller subword units present in the learned vocabulary, allowing flexible encoding."}, {"question": "Consider a scenario where a query 'teach multtimedia' is segmented into sub-sequences for retrieval. Which feature of sub-word tokenization allows partial matching even with misspellings?", "options": ["N-gram construction with stop-word filtering", "Frequency-based vocabulary expansion", "Sub-sequences with optional token proximity consideration", "Embedding layers mapping tokens to dense vectors"], "correct": 2, "explanation": "Segmenting words into overlapping sub-sequences allows retrieval models to match most sub-sequences even if the word is misspelled, enabling robust partial matching."}, {"question": "What is a common effect of converting text to lowercase before tokenization?", "options": ["It always improves grammatical correctness of generated text", "It prevents the model from distinguishing between 'Apple' and 'apple'", "It increases the vocabulary size", "It encodes semantic similarity between numbers"], "correct": 1, "explanation": "Lowercasing helps reduce vocabulary size and merge tokens that differ only by case, but it prevents the model from generating proper capitalization."}, {"question": "Why are embedding layers preferred over one-hot vectors in large language models?", "options": ["They completely eliminate the need for tokenization", "They reduce dimensionality and capture semantic similarity", "They encode only the order of tokens, not meaning", "They allow direct input of raw text into transformers"], "correct": 1, "explanation": "Embedding layers map sparse one-hot vectors to lower-dimensional dense vectors, capturing semantic relationships efficiently."}, {"question": "In information retrieval, what is the trade-off when expanding queries with synonyms or hypernyms?", "options": ["Improved precision but reduced recall", "Improved recall but potentially reduced precision", "No effect on either recall or precision", "Guaranteed improvement in both recall and precision"], "correct": 1, "explanation": "Expanding queries increases the chance of retrieving relevant documents (higher recall) but may introduce unrelated items (lower precision)."}, {"question": "When applying PMI to rank bi-grams, which scenario yields the highest PMI score?", "options": ["When a bi-gram appears frequently but its individual terms appear elsewhere often", "When a bi-gram appears exclusively together with minimal occurrences", "When a bi-gram contains common stop words", "When a bi-gram has unequal frequencies between its terms"], "correct": 1, "explanation": "PMI favors bi-grams where the terms appear almost exclusively together, making them statistically significant."}, {"question": "Which stemming algorithm in English is described as the most aggressive, often producing overly short stems that may collide with unrelated words?", "options": ["Porter Stemmer", "Lancaster Stemmer", "Snowball Stemmer", "WordNet Lemmatizer"], "correct": 1, "explanation": "The Lancaster Stemmer is aggressive in removing suffixes, sometimes reducing different words to the same short stem."}, {"question": "In text retrieval, why might dictionary-based stemmers like WordNet and spaCy be preferred over rule-based stemmers?", "options": ["They run faster than rule-based stemmers", "They produce stems that are linguistically correct", "They always generate shorter stems", "They ignore part-of-speech information"], "correct": 1, "explanation": "WordNet and spaCy produce linguistically correct stems, ensuring that variants like 'had' and 'have' map to the same base form."}, {"question": "What distinguishes endocentric compounds from exocentric compounds?", "options": ["Endocentric compounds have a semantic head, exocentric do not", "Exocentric compounds are shorter than endocentric compounds", "Endocentric compounds occur only in German and Finnish", "Exocentric compounds always use binding syllables"], "correct": 0, "explanation": "Endocentric compounds derive meaning from a head element, whereas exocentric compounds' meaning cannot be inferred from their parts."}, {"question": "How is the best split of a compound determined when multiple options exist?", "options": ["By choosing the split with the fewest components", "By using a random selection among valid splits", "By selecting the split with the highest average log-frequency of components", "By taking the split with the longest component"], "correct": 2, "explanation": "The algorithm chooses the split whose components have the highest average log-frequency in the corpus, representing the most probable split."}, {"question": "Why are synonyms problematic in information retrieval systems?", "options": ["They reduce search speed", "They prevent matching query terms to semantically equivalent document terms", "They increase spelling errors", "They break POS tagging"], "correct": 1, "explanation": "Different words with similar meanings, like 'buy' and 'purchase', may not match in documents unless synonym expansion is used."}, {"question": "In text retrieval, why is stem correctness less critical than mapping variants to the same token?", "options": ["Because incorrect stems always improve search results", "Because all algorithms produce identical stems", "Because mapping variants ensures that different forms of a word are recognized as the same token", "Because stemming is irrelevant for English text"], "correct": 2, "explanation": "Ensuring that different inflected forms map to the same stem allows queries to match more document variants."}, {"question": "Which algorithm type generally produces linguistically correct stems for English words?", "options": ["Rule-based stemmers like Porter and Lancaster", "Dictionary-based approaches like WordNet and spaCy", "Snowball without modifications", "Lancaster only"], "correct": 1, "explanation": "WordNet and spaCy use dictionary-based approaches, producing stems that are linguistically valid."}, {"question": "How does NER improve the interpretation of queries like 'Who is Albert Einstein?' in search engines?", "options": ["By filtering out stop words from the query", "By assigning POS tags only", "By collapsing relevant tokens into a named entity and inferring user intent", "By correcting spelling errors automatically"], "correct": 2, "explanation": "NER identifies 'Albert Einstein' as a single named entity, helping the search system understand that the user seeks information about a person."}, {"question": "How do large language models with a closed vocabulary handle new or uncommon words, such as novel names, during tokenization?", "options": ["They ignore the words completely, leading to missing information in the model", "They break the words into subword units or byte-pair encoded tokens present in the vocabulary", "They automatically add the new words to the model's vocabulary during inference", "They replace all unknown words with a generic placeholder token regardless of context"], "correct": 1, "explanation": "LLMs with closed vocabularies use subword or byte-pair encoding to split unseen words into smaller units present in the vocabulary, allowing them to process novel names without adding new tokens."}]}}