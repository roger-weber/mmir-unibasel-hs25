{"meta-data": {"title": "Multimedia Retrieval HS25", "description": "Exercise questions for MMIR HS25, University Basel"}, "questions": {"01 - Introduction": [{"question": "Which key technological shift allowed large-scale information retrieval to emerge in the 1970s?", "options": ["Development of GPU-based parallel computing platforms", "Rise of computer typesetting and word processing creating machine-readable text", "Introduction of transformer architectures", "Creation of cloud-based storage services"], "correct": 1, "explanation": "The 1970s saw widespread computer typesetting and word processing, producing machine-readable text that enabled large-scale retrieval systems."}, {"question": "What major innovation distinguished Google's PageRank from earlier web search algorithms?", "options": ["It indexed only multimedia content instead of text.", "It ranked pages by the authority of inbound links rather than just keyword matching.", "It eliminated the need for any ranking and returned results in random order.", "It introduced manual tagging for every webpage."], "correct": 1, "explanation": "PageRank ranked pages based on the authority of links pointing to them, moving beyond pure keyword matching."}, {"question": "Why is the 'semantic gap' particularly challenging in multimedia retrieval?", "options": ["Because low-level features like pixels or frequencies do not directly represent human-level concepts.", "Because text retrieval research is more established and advanced.", "Because storage devices cannot handle multimedia file sizes.", "Because queries in multimedia retrieval are slower to process."], "correct": 0, "explanation": "The semantic gap refers to the mismatch between low-level media features and high-level human semantics, making direct matching difficult."}, {"question": "Which retrieval architecture first combines a retriever with a language model to generate a full answer rather than extracting a single passage?", "options": ["Retriever-Reader", "Retriever-Generator (RAG)", "Retriever-Filter", "Generator-only"], "correct": 1, "explanation": "Retriever-Generator, also known as Retrieval-Augmented Generation (RAG), combines a retriever with a generative model to produce comprehensive answers."}, {"question": "Suppose you need to design a retrieval system that can infer user interests without explicit queries, as in TikTok recommendations. Which relevance strategy is most appropriate?", "options": ["Objective relevance using keyword matching", "Manual tagging and classification of all content", "Implicit relevance estimation from behavioral signals and context", "Only inverse document frequency scoring"], "correct": 2, "explanation": "Query-less search relies on behavioral signals and contextual data to infer user intent and estimate relevance without explicit queries."}, {"question": "Given the fastest PCI-E 5.0 NVMe SSD read rate of about 14,000 MB/s, approximately how long would it take to sequentially read 1 petabyte of data?", "options": ["Around 20 hours", "Around 1 day", "Around 8 days", "Around 1 year"], "correct": 2, "explanation": "1 petabyte is about 1,000,000,000 MB; at 14,000 MB/s it would take roughly 71,000 seconds (~20 hours) per 1 TB, so about 8 days for 1 PB."}, {"question": "Which retrieval architecture simply returns documents matching a query without explicit ranking?", "options": ["Retriever-Only", "Retriever-Ranker", "Retriever-Reader", "Retriever-Generator"], "correct": 0, "explanation": "Retriever-only systems fetch and present matching documents directly without filtering or ranking stages."}, {"question": "Why is searching images inherently more complex than searching text, according to the concept of the semantic gap?", "options": ["Images require more storage space than text", "Pixels have no fixed, direct relation to high-level concepts", "Image queries always need manual tagging", "Text search engines cannot process metadata"], "correct": 1, "explanation": "The semantic gap is the mismatch between low-level image features and the human semantics they represent."}, {"question": "Which statement best describes Retrieval-Augmented Generation (RAG)?", "options": ["It relies solely on a language model without external information", "It retrieves relevant documents and combines them with a query for a generative model to answer", "It uses Boolean logic to refine search results", "It is limited to ranking web pages by link authority"], "correct": 1, "explanation": "RAG retrieves supporting documents and feeds them with the query into a language model to generate a comprehensive answer."}, {"question": "The exponential data growth described in the text implies which critical challenge for retrieval systems?", "options": ["Lower hardware costs make retrieval trivial", "Indexing and ranking must occur nearly in real time to remain relevant", "Users no longer require high recall", "Query-less search becomes impossible"], "correct": 1, "explanation": "Rapidly created data such as news and social posts require near real-time indexing and ranking to stay relevant."}], "02 - Classical Retrieval": [{"question": "In the classical Boolean retrieval model, how is a document's relevance to a query determined?", "options": ["By calculating the cosine similarity between document and query vectors", "By checking if the document satisfies the Boolean expression of the query", "By estimating the probability that the document is relevant", "By counting the frequency of each query term"], "correct": 1, "explanation": "The Boolean model includes a document if it satisfies the Boolean query expression, without using ranking or probability."}, {"question": "Which representation records how often each term appears but ignores the order of terms?", "options": ["Set-of-words model", "Bag-of-words model", "Vector embedding model", "Probabilistic model"], "correct": 1, "explanation": "The bag-of-words model preserves term frequencies but discards word order and proximity."}, {"question": "What is a primary motivation for chunking long documents into smaller units before indexing?", "options": ["To reduce the size of the vocabulary", "To eliminate the need for metadata extraction", "To improve retrieval precision by ensuring query terms occur closer together", "To avoid the need for an inverted index"], "correct": 2, "explanation": "Chunking forces query terms to appear within the same segment, which improves precision by reducing false positives due to distant term occurrences."}, {"question": "Which feature distinguishes faceted search from basic Boolean retrieval?", "options": ["Faceted search requires all query terms to be present", "Users can add or remove filters and sort results without resubmitting the search", "It relies solely on probabilistic ranking of documents", "It eliminates the need for metadata attributes"], "correct": 1, "explanation": "Faceted search allows interactive filtering and sorting independent of the original query, improving navigation of large result sets."}, {"question": "Why are stop words like \"the\" often given very low weight in modern retrieval systems?", "options": ["They rarely appear in documents", "They significantly increase the size of the index", "They have high document frequency and low discriminative power", "They cannot be tokenized correctly"], "correct": 2, "explanation": "Stop words appear in most documents and thus cannot differentiate relevant from non-relevant documents, reducing their discriminative power."}, {"question": "In the indexing pipeline's offline phase, which step directly produces high-dimensional feature vectors for retrieval?", "options": ["Tokenization", "Summarization", "Metadata extraction", "Chunk splitting"], "correct": 1, "explanation": "During summarization, tokens are transformed into high-dimensional feature representations used for indexing and later retrieval."}, {"question": "The discrimination power dp(t) of a term t, as defined by Salton, Wong, and Yang (1975), is high when:", "options": ["Removing t causes documents to become more similar to the collection centroid", "Term t occurs in almost all documents", "Term t has low inverse document frequency", "The similarity to the centroid decreases upon removing t"], "correct": 0, "explanation": "A high dp(t) means that removing the term increases the similarity of documents to the centroid, showing that the term previously helped to distinguish documents."}, {"question": "Which key property makes text retrieval less affected by the semantic gap compared to other media?", "options": ["It always uses structured metadata", "It directly matches user text queries to terms in unstructured documents", "It relies on image recognition algorithms", "It requires manual annotation of every document"], "correct": 1, "explanation": "Text retrieval matches user-entered text queries directly to terms in unstructured documents, minimizing issues caused by semantic interpretation."}, {"question": "What is the main advantage of Boolean retrieval systems when scanning data?", "options": ["They can score and rank results in real time", "They determine each document's relevance independently without post-processing", "They automatically expand queries with synonyms", "They always outperform vector-space models"], "correct": 1, "explanation": "Boolean retrieval determines relevance for each document independently and does not require a post-processing ranking step."}, {"question": "In the set-of-words model, how is a document represented?", "options": ["As a sequence of characters with full formatting", "As a set of unique terms, ignoring order and frequency", "As a weighted graph of co-occurring phrases", "As a probability distribution of word embeddings"], "correct": 1, "explanation": "The set-of-words model records only the presence of unique terms, ignoring both order and frequency."}, {"question": "Why was faceted search introduced as an extension to the Boolean model?", "options": ["To enable partial matches of query terms", "To improve tokenization of complex documents", "To allow users to filter and sort large result sets interactively", "To compute inverse document frequency scores"], "correct": 2, "explanation": "Faceted search was added to help users navigate large result sets by applying filters and sorting without changing document relevance."}, {"question": "What key improvement did the Extended Boolean Model introduce over the classical Boolean Model?", "options": ["Support for natural language queries", "Partial matching and term-frequency-based relevance scoring", "Integration of hyperlink analysis", "Automatic language translation"], "correct": 1, "explanation": "The Extended Boolean Model allows partial matches and uses term frequency to assign relevance scores, enabling ranked results."}, {"question": "In the offline indexing phase, what is the primary purpose of the 'split' step?", "options": ["To tokenize text into words and subwords", "To divide long documents into smaller coherent retrieval units", "To normalize character encodings to UTF-8", "To rank documents by retrieval status value"], "correct": 1, "explanation": "During the split step, long documents are divided into smaller searchable chunks, enabling finer-grained retrieval."}, {"question": "Why is overlapping text sometimes added when chunking long documents?", "options": ["To improve storage compression", "To avoid false negatives when query terms span adjacent chunks", "To simplify vector-space calculations", "To increase the inverse document frequency of rare terms"], "correct": 1, "explanation": "Overlapping text ensures that query terms at chunk boundaries are captured together, reducing false negatives."}, {"question": "Which factor best explains why terms with very low or very high document frequency have low discrimination power?", "options": ["They cannot be tokenized properly", "They fail to differentiate documents effectively", "They have high inverse document frequency", "They are always removed as stop words"], "correct": 1, "explanation": "Terms appearing in too few or too many documents provide little ability to distinguish relevant from non-relevant documents."}, {"question": "Which key limitation of the Standard Boolean Model motivated the development of ranked retrieval methods?", "options": ["Lack of partial match capability", "Excessive computational complexity", "Dependence on term frequencies", "Requirement of probabilistic feedback"], "correct": 0, "explanation": "The Standard Boolean Model returns only documents that fully satisfy the Boolean query and cannot rank or partially match documents, leading to either too many or too few results."}, {"question": "Why does the cosine similarity measure reduce the bias toward longer documents compared to the inner product?", "options": ["It ignores inverse document frequency", "It normalizes vectors by their length", "It uses only binary term presence", "It excludes query term frequencies"], "correct": 1, "explanation": "Cosine similarity divides by the product of vector lengths, so document length does not directly inflate similarity scores."}, {"question": "Which assumption is fundamental to the Binary Independence Model?", "options": ["Documents are represented as dense real-valued vectors", "Term independence and binary term presence", "Relevance depends on query term proximity", "Term frequencies must be normalized"], "correct": 1, "explanation": "The BIR assumes terms occur independently and represent documents as binary (present/absent) sets, disregarding exact frequencies."}, {"question": "How does BM25 address the issue of very frequent query terms dominating the score?", "options": ["By discarding all terms appearing in more than 50% of documents", "By applying a saturation function to term frequency", "By using only binary term weights", "By removing inverse document frequency entirely"], "correct": 1, "explanation": "BM25 uses a nonlinear saturation of term frequency controlled by parameter k, preventing excessive influence of very frequent terms."}, {"question": "What role does the parameter b play in the BM25 scoring formula?", "options": ["Controls how document length normalizes term frequency", "Determines the inverse document frequency smoothing", "Sets the minimum similarity threshold for retrieval", "Specifies the user feedback weight in relevance estimation"], "correct": 0, "explanation": "The b parameter adjusts the impact of document length on term frequency saturation, favoring shorter or longer documents depending on its value."}, {"question": "A researcher wants a retrieval model that (1) provides a probabilistic foundation, (2) allows partial matches, (3) incorporates term frequency with diminishing returns, and (4) accounts for document length. Which model best satisfies all these requirements?", "options": ["Standard Boolean Model", "Vector Space Model with cosine similarity", "Binary Independence Model", "BM25"], "correct": 3, "explanation": "BM25 integrates probabilistic principles, supports partial matching, applies saturated term frequency, and normalizes by document length, fulfilling all four criteria."}, {"question": "Which statement best describes the primary limitation of the Standard Boolean Model in information retrieval?", "options": ["It requires complex probabilistic calculations.", "It cannot rank documents by relevance.", "It depends on user feedback to estimate probabilities.", "It only works with normalized vectors."], "correct": 1, "explanation": "The Standard Boolean Model only filters documents as relevant or not without ranking them by relevance."}, {"question": "What key advantage does the Vector Space Model have over the Standard Boolean Model?", "options": ["It guarantees independent term assumptions.", "It provides ranked retrieval based on similarity measures.", "It avoids using term frequencies entirely.", "It uses only Boolean logic for query processing."], "correct": 1, "explanation": "The Vector Space Model ranks documents by computing similarity between query and document vectors."}, {"question": "In the Vector Space Model, which measure computes the angle between the query and document vectors?", "options": ["Inner product", "Bayes' theorem", "Cosine similarity", "P-norm model"], "correct": 2, "explanation": "The cosine measure calculates the angle between query and document vectors to assess similarity."}, {"question": "According to the Binary Independence Model, which assumption is made about terms absent from the query?", "options": ["They are given higher weights.", "They do not affect document ranking.", "They are normalized using idf values.", "They must be present in all relevant documents."], "correct": 1, "explanation": "The BIR model assumes that terms absent from the query are equally distributed in relevant and non-relevant documents and thus do not affect ranking."}, {"question": "Which feature distinguishes the Extended Boolean Model from the Vector Space Model despite both supporting ranked retrieval?", "options": ["The Extended Boolean Model uses Boolean query structure while Vector Space treats queries as vectors.", "The Extended Boolean Model uses cosine similarity while Vector Space does not.", "The Extended Boolean Model ignores idf weighting while Vector Space requires it.", "The Extended Boolean Model relies on Bayes' theorem for probability estimation."], "correct": 0, "explanation": "Extended Boolean retains Boolean expressions for queries, unlike the Vector Space Model which treats queries as weighted vectors."}, {"question": "Consider a retrieval scenario with very frequent terms appearing in more than 50% of documents. According to the BM25 derivation from the BIR model, what happens to the idf-like weight c_j for such terms and why is this significant for ranking?", "options": ["c_j becomes zero, ensuring these common terms have no influence on ranking.", "c_j becomes negative, potentially lowering the score contribution of very common terms.", "c_j grows without bound, dominating the score.", "c_j remains constant, giving frequent terms equal weight to rare terms."], "correct": 1, "explanation": "For very frequent terms, the BM25/BIR-derived c_j can be negative, reducing their contribution and preventing common terms from dominating the ranking."}, {"question": "In the Binary Independence Model, why can terms absent from the query be ignored in ranking?", "options": ["They have zero inverse document frequency", "Their occurrence probability is assumed equal in relevant and non-relevant documents", "They are treated as stop words by default", "They cancel out during cosine normalization"], "correct": 1, "explanation": "BIR assumes terms not in the query have the same probability in relevant and non-relevant documents, so they do not affect ranking."}, {"question": "Suppose a corpus contains extremely long documents where relevant terms are scattered far apart. Which combination of strategies best improves both precision and recall without excessive index growth?", "options": ["Use only fixed-size chunking without overlap", "Apply structural chunking with overlapping windows", "Rely solely on term stemming", "Increase BM25's k parameter and ignore chunking"], "correct": 1, "explanation": "Structural chunking preserves semantic boundaries while overlapping windows reduce false negatives, balancing precision and recall for long documents."}, {"question": "Compare how the Extended Boolean Model, Vector Space Model, and BM25 each handle term frequency when ranking documents. Which statement best describes their key differences?", "options": ["All three ignore term frequency entirely, focusing only on binary presence of terms.", "Extended Boolean uses term frequency to assign similarity scores, Vector Space weights terms by tf-idf without saturation, and BM25 adds frequency saturation and length normalization.", "Extended Boolean and Vector Space use tf-idf with saturation, while BM25 ignores term frequency.", "Vector Space and BM25 both ignore term frequency but normalize for document length."], "correct": 1, "explanation": "Extended Boolean incorporates term frequency for partial matches, Vector Space uses tf-idf without saturation, and BM25 introduces frequency saturation with document length normalization."}, {"question": "Which best describes the relationship between the offline indexing pipeline and linguistic preprocessing like stemming or lemmatization?", "options": ["Stemming is performed after the index is built to reduce vector dimensionality.", "Tokens are stemmed or lemmatized during feature extraction so the index stores normalized terms for matching queries.", "Lemmatization is only applied at query time and never during indexing.", "The inverted index itself performs stemming dynamically for each query."], "correct": 1, "explanation": "During the offline phase, stemming or lemmatization occurs in the tokenization/feature extraction stage so the index uses normalized terms."}, {"question": "How does user relevance feedback in the Binary Independence Model (BIR) relate to BM25's scoring approach?", "options": ["Feedback adjusts BM25's k and b hyperparameters directly.", "BIR uses feedback to refine probabilities r_j and n_j, similar to how BM25 adjusts IDF weights but without direct feedback loops.", "BM25 requires feedback to estimate IDF values just like BIR.", "Both models ignore user feedback entirely."], "correct": 1, "explanation": "BIR iteratively updates r_j and n_j using feedback, conceptually similar to BM25's probabilistic weighting though BM25 does not require explicit feedback."}, {"question": "When incorporating HTML metadata and anchor text into a BM25-based retrieval engine, which weighting strategy is most appropriate to counteract spammy anchor texts?", "options": ["Assign extremely high weights to all anchor text terms to ensure recall.", "Treat anchor text terms with the same weight as body text regardless of context.", "Index anchor text but down-weight overly common or misleading phrases to limit click-bait influence.", "Exclude all anchor text from the index to avoid bias."], "correct": 2, "explanation": "Anchor text is valuable but can be manipulated; indexing with cautious down-weighting mitigates spam without losing relevance signals."}, {"question": "How can Zipf's law guide stop-word removal and IDF weighting for a multilingual corpus to improve both Boolean and Vector Space retrieval?", "options": ["By enforcing equal frequency of all terms across languages.", "By identifying extremely high- and low-frequency words for potential down-weighting or exclusion, refining IDF weighting.", "By ensuring rare terms are always removed to reduce vocabulary size.", "By ranking terms purely by alphabetical order to maintain neutrality."], "correct": 1, "explanation": "Zipf's law highlights very frequent and very rare terms, informing stop-word lists and IDF weighting to balance discrimination and recall."}]}}