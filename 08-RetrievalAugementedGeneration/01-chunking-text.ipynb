{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk text into smaller parts\n",
    "\n",
    "Retrieving information from larger texts requires locating relevant information with the teyt body. To provide a finer granluarity to find information, we can split the document into smaller parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from helpers import get_book, get_pdf, print_table\n",
    "from customsplitter import CustomSplitterParagraphs, CustomSplitterSimilarity\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    TextSplitter, CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter, SpacyTextSplitter,\n",
    "    Language\n",
    ")\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import re, itertools\n",
    "\n",
    "# turn off warnings\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    ('A Study in Scarlet (en)', 244), \n",
    "    ('Pride and Prejudice (en)', 1342),\n",
    "    ('Les Misérables (en)', 135),\n",
    "    ('Buddenbrooks: Verfall einer Familie (de)', 34811),\n",
    "    ('Les trois mousquetaires (fr)', 13951),\n",
    "    ('Bajki (pl)', 27729),\n",
    "    ('Buddah (zn)', 23585),\n",
    "    ('Chapter 1: Introduction (en)', \"pdfs/01_Introduction.pdf\"), \n",
    "    ('Chapter 2: Evaluation (en)', \"pdfs/02_ClassicalTextRetrieval.pdf\"),\n",
    "    ('Chapter 3: Classical Text Retrieval (en)', \"pdfs/03_PerformanceEvaluation.pdf\"),\n",
    "    ('Chapter 4: Advanced Text Retrieval (en)', \".pdfs/04_AdvancedTextProcessing.pdf\"),\n",
    "]\n",
    "\n",
    "# initialize splitters if not yet defined\n",
    "if 'splitters' not in globals():\n",
    "    splitters = {}\n",
    "out_stats = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '40%'})\n",
    "out_plot = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '50%'})\n",
    "out_samples = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '50%'})\n",
    "\n",
    "# split and print results\n",
    "def split_and_print(docs: Document | List[Document], splitter: TextSplitter, chunk_ids: list[int] = None):\n",
    "    if isinstance(docs, Document):\n",
    "        docs = [docs]\n",
    "    chunks = [(x.page_content, x.metadata['id']) for x in splitter.split_documents(docs)]\n",
    "    with out_stats:\n",
    "        doc_len = sum([len(doc.page_content) for doc in docs])\n",
    "        clear_output()\n",
    "        print(f'doucment length: {doc_len}')\n",
    "        print(f'number of pages: {len(docs)}')\n",
    "        print(f' document parts: {len(chunks)}')\n",
    "        print(f'   sum of parts: {sum([len(part[0]) for part in chunks])}')\n",
    "        print(f' parts overhead: {sum([len(part[0]) for part in chunks]) / doc_len:.2f}')\n",
    "        print(f'avg part length: {sum([len(part[0]) for part in chunks]) / len(chunks):.2f}\\n')\n",
    "    with out_plot:\n",
    "        clear_output()\n",
    "        df = DataFrame([len(p[0]) for p in chunks], columns = ['length'])\n",
    "        sns.displot(df, x='length', bins=50, height=3, aspect=2)\n",
    "        plt.show()\n",
    "    with out_samples:\n",
    "        clear_output()\n",
    "        print_table([(f'{chunks[i][1]}:{i+1}', re.sub(r'\\s+',' ', chunks[i][0])) for i in chunk_ids if i<len(chunks)],['chunk', 'text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different splitting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Splitting the text into fixed-sized chunks (word boundaries)\n",
    "A simple way to chunk text is to split it into words, and then merge words until the resulting chunks reach a certain size. To remediate the impact of breaking sentences or paragraphs in the middle, we can overlap subsequent \n",
    "chunks by a defined number of words. This way, even if a paragraph is split, it is likely to be retained with the next chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters[\"words\"] = lambda size, overlap: CharacterTextSplitter(        \n",
    "    separator = \" \",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = overlap,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Splitting at sentence boundaries\n",
    "Similar to the previous approach, we use sentences as the smallest units of text for chunking. This avoids abrupt sentence breaks but may introduce slight variations in chunk sizes, which are usually negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters[\"sentence (nltk)\"] = lambda size, overlap: NLTKTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = overlap,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters[\"sentence (spacy)\"] = lambda size, overlap: SpacyTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = overlap,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: splitting on structure\n",
    "\n",
    "The concept is to divide text based on its structural elements. For instance, in books, we can split at pages, parts, chapters, sections, and paragraphs. Authors commonly use these structural elements to separate content, making them strong indicators of topic or aspect changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters[\"chapters\"] = lambda size, overlap: CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\\n\\n\",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = overlap,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters[\"paragraphs\"] = lambda size, overlap: RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\\n\\n\", \"\\n\\n\\n\", \"\\n\\n\"],\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = overlap,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: semantic splitting\n",
    "\n",
    "The structural approach is often the simplest way to obtain semantically coherent chunks. However, when reliable structural context extraction is challenging (e.g., in web pages with varying header formats or scanned documents), we can extend the sentence-based splitting method: \n",
    "1) Define a similarity measure between sentences. \n",
    "2) Set minimum and maximum chunk sizes. \n",
    "3) Split the text into sentences using NLTK or spaCy (merge very short sentences to meet the minimum size). \n",
    "4) Merge neighboring chunks if they are similar (while ensuring they don't exceed the maximum size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters[\"custom-paragraph\"] = lambda size, overlap: CustomSplitterParagraphs()\n",
    "splitters[\"custom-similarity\"] = lambda size, overlap: CustomSplitterSimilarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd10fc07331493aac9b59c595ba1e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(Dropdown(description='Text source:', layout=Layout(width='500px'), option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chunk_text(source, splitter, size, overlap):\n",
    "    clear_output()\n",
    "    docs = None\n",
    "    if isinstance(source, str):\n",
    "        docs = get_pdf(source)\n",
    "    elif isinstance(source, int):\n",
    "        docs = [get_book(source)]\n",
    "    if docs:\n",
    "        split_and_print(docs, splitters[splitter](size, overlap), [5,6,7,8,9,25,26,27,28,29]) \n",
    "\n",
    "form_source = widgets.Dropdown(\n",
    "    options=sources,\n",
    "    value=sources[0][1],\n",
    "    description='Text source:',\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '150px'},\n",
    ")\n",
    "form_splitter = widgets.Dropdown(\n",
    "    options=splitters.keys(),\n",
    "    value=list(splitters.keys())[0],\n",
    "    description='Text splitter:',\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '150px'},\n",
    ")\n",
    "form_size = widgets.IntSlider(min=200, max=10000, value=1000, step=200, description='Chunk size:')\n",
    "form_overlap = widgets.IntSlider(min=0, max=1000, value=200, step=50, description='Overlap:')\n",
    "form_input = widgets.interactive(chunk_text, source=form_source, splitter=form_splitter, size=form_size, overlap=form_overlap)\n",
    "display(widgets.VBox([form_input, out_stats, out_plot, out_samples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical chunking\n",
    "\n",
    "For RAG use cases, we aim to locate relevant chunks in our library and incorporate them into our prompt alongside the user's query. Embeddings (which we will discuss later) are most effective with smaller chunks, while text generation works better with more context. Earlier models had much smaller context windows, but modern language models have 100k or even 1m token context, allowing for much more information to be included.\n",
    "\n",
    "The concept of hierarchical chunking involves using a larger chunk for context and a smaller chunk within that for retrieval. When a smaller chunk is found, it fills in the larger context. We can also add more context by including preceding and succeeding chunks, similar to how humans scan a book for more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_SIZE = 30000\n",
    "MIN_CONTEXT_SIZE = 1000\n",
    "\n",
    "# initialize splitters if not yet defined\n",
    "if 'splitters_context' not in globals():\n",
    "    splitters_context = {}\n",
    "    splitters_search = {}\n",
    "\n",
    "splitters_context[\"chapters\"] = lambda: CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\\n\\n\",\n",
    "    chunk_size = MIN_CONTEXT_SIZE,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters_context[\"paragraphs\"] = lambda: RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\\n\\n\", \"\\n\\n\\n\", \"\\n\\n\"],\n",
    "    chunk_size = MIN_CONTEXT_SIZE,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters_context['pages'] = lambda: CharacterTextSplitter(\n",
    "    separator = \" \",\n",
    "    chunk_size = MAX_CONTEXT_SIZE,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters_search[\"words\"] = lambda size: CharacterTextSplitter(        \n",
    "    separator = \" \",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "splitters_search[\"sentence (nltk)\"] = lambda size: NLTKTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "splitters_search[\"sentence (spacy)\"] = lambda size: SpacyTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = size,\n",
    "    chunk_overlap  = 0,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting document hierarchically and print stats, plots, and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_stats_h = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '40%'})\n",
    "out_plot_h = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '50%'})\n",
    "out_samples_h = widgets.Output(layout = {'padding': '0px 50px', 'min_width': '50%'})\n",
    "\n",
    "# split and print results\n",
    "def split_hierarchically_and_print(docs: Document | List[Document], splitter_context: TextSplitter, splitter_search: TextSplitter, chunk_ids: list[int] = None):\n",
    "    contexts = [(x.page_content, x.metadata['id'], []) for i, x in enumerate(splitter_context.split_documents(docs))]\n",
    "    for context in contexts:\n",
    "        context[2].extend(splitter_search.split_text(context[0]))\n",
    "\n",
    "    with out_stats_h:\n",
    "        clear_output()\n",
    "        doc_len = sum([len(doc.page_content) for doc in docs])\n",
    "        print(f'   doucment length: {doc_len}')\n",
    "        print(f'      num contexts: {len(contexts)}')\n",
    "        print(f' num search chunks: {sum([len(c[2]) for c in contexts])}')\n",
    "        print(f'avg context length: {sum([len(c[0]) for c in contexts]) / len(contexts):.2f}')\n",
    "        print(f'  avg chunk length: {sum([sum(len(p) for p in c[2]) for c in contexts]) / sum([len(c[2]) for c in contexts]):.2f}\\n')\n",
    "    with out_plot_h:\n",
    "        clear_output()\n",
    "        df = DataFrame([len(c[0]) for c in contexts], columns = ['length'])\n",
    "        sns.displot(df, x='length', bins=50, height=3, aspect=2)\n",
    "        plt.show()\n",
    "        df = DataFrame([len(c) for c in itertools.chain(*[c[2] for c in contexts])], columns = ['length'])\n",
    "        sns.displot(df, x='length', bins=50, height=3, aspect=2)\n",
    "        plt.show()\n",
    "    with out_samples_h:\n",
    "        clear_output()\n",
    "        print_table([(f'{contexts[i][1]}:{i+1}', '<hr>'.join([re.sub(r'\\s+',' ', c) for c in contexts[i][2]])) for i in chunk_ids if i<len(contexts)],['contexts', 'search chunks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f879b7f9dc64a1abb68492d75533d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(interactive(children=(Dropdown(description='Text source:', layout=Layout(width='500px'), option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chunk_text(source, splitter_context, splitter_search, size):\n",
    "    clear_output()\n",
    "    docs = None\n",
    "    if isinstance(source, str):\n",
    "        docs = get_pdf(source)\n",
    "    elif isinstance(source, int):\n",
    "        docs = [get_book(source)]\n",
    "    if docs:\n",
    "        split_hierarchically_and_print(docs, splitters_context[splitter_context](), splitters_search[splitter_search](size), [4,9,14]) \n",
    "\n",
    "form_source_h = widgets.Dropdown(\n",
    "    options=sources,\n",
    "    value=sources[0][1],\n",
    "    description='Text source:',\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '150px'},\n",
    ")\n",
    "form_splitter_context = widgets.Dropdown(\n",
    "    options=splitters_context.keys(),\n",
    "    value=list(splitters_context.keys())[0],\n",
    "    description='Context splitter:',\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '150px'},\n",
    ")\n",
    "form_splitter_search = widgets.Dropdown(\n",
    "    options=splitters_search.keys(),\n",
    "    value=list(splitters_search.keys())[0],\n",
    "    description='Search splitter:',\n",
    "    layout=widgets.Layout(width='500px'),\n",
    "    style={'description_width': '150px'},\n",
    ")\n",
    "form_size_h = widgets.IntSlider(min=200, max=2000, value=400, step=100, description='Chunk size:')\n",
    "form_input_h = widgets.interactive(chunk_text, source=form_source, splitter_context=form_splitter_context, splitter_search=form_splitter_search, size=form_size_h)\n",
    "display(widgets.VBox([form_input_h, out_stats_h, out_plot_h, out_samples_h]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
