{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, random\n",
    "\n",
    "# get started with data and setting\n",
    "tweets = ([(t, \"pos\") for t in nltk.corpus.twitter_samples.strings(\"positive_tweets.json\")] +\n",
    "                            [(t, \"neg\") for t in nltk.corpus.twitter_samples.strings(\"negative_tweets.json\")])\n",
    "new_tweets = [re.sub('\\||\\\\n', '', t) for t in nltk.corpus.twitter_samples.strings('tweets.20150430-223406.json')]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "\n",
    "# print stats and head of data\n",
    "n_tweets, n_pos_tweets, n_neg_tweets = len(tweets), len([t for t in tweets if t[1] == \"pos\"]), len([t for t in tweets if t[1] == \"neg\"])\n",
    "print(f\"length of tweets: {n_tweets}, positive tweets: {n_pos_tweets}, negative tweets: {n_neg_tweets}\\n\")\n",
    "for d in tweets[:6]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning all the tweets --> set of words model\n",
    "def set_of_words(text):\n",
    "    # remove http links and user references\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text)\n",
    "    text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text)\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [stemmer.stem(t) for t in tokens if len(t)>1 and not t.isnumeric() and t not in stopwords]\n",
    "    # set of words\n",
    "    return {t:1 for t in tokens}\n",
    "\n",
    "data = [(set_of_words(text), label) for text, label in tweets]\n",
    "for d in data[:6]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and test (stratify pos/neg samples)\n",
    "pos_data = [x for x in data if x[1] == 'pos']\n",
    "pos_tweets = [x[0] for x in tweets if x[1] == 'pos']\n",
    "neg_data = [x for x in data if x[1] == 'neg']\n",
    "neg_tweets = [x[0] for x in tweets if x[1] == 'neg']\n",
    "\n",
    "pos_split = 80 * len(pos_data) // 100\n",
    "neg_split = 80 * len(neg_data) // 100\n",
    "\n",
    "train_data = pos_data[:pos_split] + neg_data[:neg_split]\n",
    "train_tweets = pos_tweets[:pos_split] + neg_tweets[:neg_split]\n",
    "test_data = pos_data[pos_split:] + neg_data[neg_split:]\n",
    "test_tweets = pos_tweets[pos_split:] + neg_tweets[neg_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify with Naive Bayes (bernoulli)\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Train Accuracy is:\", nltk.classify.accuracy(classifier, train_data))\n",
    "print(\"Test Accuracy is:\", nltk.classify.accuracy(classifier, test_data))\n",
    "print(\"\\n\")\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from tabulate import tabulate\n",
    "\n",
    "# get all false predictions\n",
    "false_predictions = [('train', classifier.classify(t[0]), t[1], re.sub('\\||\\\\n','',train_tweets[i]), ', '.join(t[0].keys())) for i, t in enumerate(train_data) if classifier.classify(t[0]) != t[1]]\n",
    "false_predictions += [('test', classifier.classify(t[0]), t[1], re.sub('\\||\\\\n','',test_tweets[i]), ', '.join(t[0].keys())) for i, t in enumerate(test_data) if classifier.classify(t[0]) != t[1]]\n",
    "print(f'false predictions: {len(false_predictions)} out of {len(tweets)}')\n",
    "\n",
    "headers = ['set', 'predicted', 'actual', 'tweet', 'tokens']\n",
    "Markdown(tabulate(false_predictions, headers, tablefmt='github'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "headers = ['predicted', 'tweet']\n",
    "random.shuffle(new_tweets)\n",
    "predictions = [classifier.classify(set_of_words(text)) for text in new_tweets[0:10]]\n",
    "Markdown(tabulate(zip(predictions, new_tweets), headers, tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# create pipeline for sentiment analysis\n",
    "trf_classifier = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "headers = ['predicted', 'score', 'actual', 'tweet']\n",
    "predictions = [trf_classifier(text)[0] for text in pos_tweets[0:10]]\n",
    "rows = [[x['label'], x['score'], 'pos', re.sub('\\||\\\\n','',pos_tweets[i])] for i,x in enumerate(predictions)]\n",
    "predictions = [trf_classifier(text)[0] for text in neg_tweets[0:10]]\n",
    "rows += [[x['label'], x['score'], 'neg', re.sub('\\||\\\\n','',neg_tweets[i])] for i,x in enumerate(predictions)]\n",
    "Markdown(tabulate(rows, headers, tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all false predictions\n",
    "pos_predicitions = trf_classifier(pos_tweets[:512])\n",
    "neg_predicitions = trf_classifier(neg_tweets[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = len([x for x in pos_predicitions if x['label']!='LABEL_0'])\n",
    "fp = len([x for x in pos_predicitions if x['label']=='LABEL_0'])\n",
    "tn = len([x for x in neg_predicitions if x['label']!='LABEL_2'])\n",
    "fn = len([x for x in neg_predicitions if x['label']=='LABEL_2'])\n",
    "\n",
    "print(f'accuracy: {(tp+tn)/(tp+tn+fp+fn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
